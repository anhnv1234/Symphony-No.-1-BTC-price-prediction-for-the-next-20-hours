import pandas as pd
import numpy as np
import os
import logging
import joblib
from sklearn.preprocessing import MinMaxScaler
import warnings 
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import sys
import math # (C·∫ßn cho "N√£o" Transformer)

# --- (M√ìN 1) "B·ªäT MI·ªÜNG" C·∫¢NH B√ÅO "R√ÅC" ---
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# --- C·∫§U H√åNH LOGGING (UTF-8) ---
try:
    sys.stdout.reconfigure(encoding='utf-8')
except AttributeError:
    pass
    
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - [PlotGenerative_V53_ALL] - %(message)s',
                    handlers=[
                        logging.FileHandler("log_05_plot_generative_V53_ALL.log", mode='w', encoding='utf-8'), 
                        logging.StreamHandler(sys.stdout)
                    ])

# --- C·∫§U H√åNH "PH√íNG TRI·ªÇN L√ÉM" V53 ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logging.info(f"ƒêang d√πng thi·∫øt b·ªã: {device} (ƒë·ªÉ 'v·∫Ω')")

# --- (A) C·∫§U H√åNH "TH·ª®C ƒÇN" (D√ôNG H√ÄNG "TH√î" 53 M√ìN V√Ä SCALER "V23") ---
MASTER_FILE_PATH_THO = os.path.join('02_Master_Data', 'btcusdt_master_data.parquet')
SCALER_FILENAME_GOC = os.path.join('01_Processed_Data', 'cvae_scaler_V23.gz')
CHART_SAVE_DIR = "05_Charts_Generative_V53_ALL" # (Th∆∞ m·ª•c "tr∆∞ng" ·∫£nh V·∫º 53 M√≥n)
os.makedirs(CHART_SAVE_DIR, exist_ok=True)

# --- (B) C·∫§U H√åNH "N√ÉO" (T√™n file "n√£o x·ªãn" 53 M√≥n) ---
DIR_MODELS = "03_Models"
CVAE_LSTM_MODEL_FILE = "cvae_decoder_V11_100PCT_{lb}_{lf}.pth" # (T·ª´ "L√≤" 1a)
TIMEGAN_GRU_MODEL_FILE = "advanced_tsgan_model_{lb}_final.pth" # (T·ª´ "L√≤" 1b)
CVAE_TRANS_MODEL_FILE = "transformer_cvae_decoder_V13_{lb}_{lf}_best.pth" # (T·ª´ "L√≤" 3)

# --- (C) C·∫§U H√åNH "B·ªò N√ÉO" (Ph·∫£i "kh·ªõp" 100% l√∫c "luy·ªán") ---
LOOKBACK = 50       
LOOKFORWARD_CVAE = 20  
LATENT_DIM_CVAE = 32 
HIDDEN_DIM_TIMEGAN = 24 
NUM_FEATURES_GOC = 53 

# (C·∫•u h√¨nh "N√£o" TCVAE)
D_MODEL = 64      
N_HEAD = 4        
NUM_ENC_LAYERS = 2 
NUM_DEC_LAYERS = 2 

# (Bi·∫øn "to√†n c·ª•c" ƒë·ªÉ "l·∫•y" th√¥ng s·ªë "unscale" H1_Close)
SCALER_CLOSE_IDX = -1
SCALER_CLOSE_MIN = 0.0
SCALER_SCALE_CLOSE = 1.0

# =========================================================================
# üí° B∆Ø·ªöC 1: "B√ä" (COPY) C√ÅC "KHU√îN N√ÉO" (CLASSES) T·ª™ 3 FILE "L√í"
# =========================================================================

# --- "KHU√îN N√ÉO" 1: CVAE-LSTM (T·ª´ file train_cvae_V11.py) ---
def sampling(args):
    z_mean, z_log_var = args
    batch = z_mean.shape[0]; dim = z_mean.shape[1]
    epsilon = torch.randn(size=(batch, dim)).to(device)
    return z_mean + torch.exp(0.5 * z_log_var) * epsilon

class CVAE_LSTM_Decoder(nn.Module):
    def __init__(self, lookback, lookforward, num_features, latent_dim, num_heads=4):
        super(CVAE_LSTM_Decoder, self).__init__()
        self.lookforward = lookforward; self.num_features = num_features; self.latent_dim = latent_dim 
        self.lstm_past_1 = nn.LSTM(num_features, 64, batch_first=True, num_layers=1)
        self.lstm_past_2 = nn.LSTM(64, 64, batch_first=True, num_layers=1)
        self.attention = nn.MultiheadAttention(embed_dim=64, num_heads=num_heads, batch_first=True) 
        self.z_to_query_upscaler = nn.Linear(latent_dim, 64)
        self.dense_combine = nn.Linear(latent_dim + 64 + 64, 64 * lookforward)
        self.lstm_gen = nn.LSTM(64, 128, batch_first=True, num_layers=1)
        self.time_dist_dense = nn.Linear(128, num_features)
        self.relu = nn.ReLU(); self.sigmoid = nn.Sigmoid()
        
    def forward(self, condition_input, latent_input):
        h_past_seq, (last_hidden_past, _) = self.lstm_past_1(condition_input)
        _, (last_hidden_past_2, _) = self.lstm_past_2(h_past_seq)
        cond_features = self.relu(last_hidden_past_2.squeeze(0)) 
        z_query_upscaled = self.relu(self.z_to_query_upscaler(latent_input)) 
        query_vector = self.relu(cond_features + z_query_upscaled) 
        query_vector = query_vector.unsqueeze(1) 
        context_vector, attn_weights = self.attention(
            query=query_vector, key=h_past_seq, value=h_past_seq
        )
        context_vector = context_vector.squeeze(1) 
        combined = torch.cat([latent_input, cond_features, context_vector], dim=1)
        x = self.relu(self.dense_combine(combined))
        x = x.view(-1, self.lookforward, 64)
        x, _ = self.lstm_gen(x)
        x = self.time_dist_dense(x)
        reconstruction = self.sigmoid(x)
        return reconstruction, attn_weights
# --- (H·∫øt "khu√¥n" CVAE-LSTM) ---

# --- "KHU√îN N√ÉO" 2: TIMEGAN-GRU (T·ª´ file train_timegan_V4.py) ---
class BaseGRU(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim, num_layers=2):
        super().__init__()
        self.rnn = nn.GRU(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)
        self.output_layer = nn.Linear(hidden_dim, output_dim)
    def forward(self, x):
        output, _ = self.rnn(x); return self.output_layer(output)

class TimeGAN_GRU_Generator(BaseGRU):
    def __init__(self, hidden_dim):
        super().__init__(hidden_dim, hidden_dim, hidden_dim)

class TimeGAN_GRU_Recovery(BaseGRU):
    def __init__(self, num_features, hidden_dim):
        super().__init__(hidden_dim, num_features, hidden_dim)
# --- (H·∫øt "khu√¥n" TimeGAN-GRU) ---

# --- "KHU√îN N√ÉO" 3: CVAE-Transformer (T·ª´ file 04_train_transformer_cvae_V1.py) ---
class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        pe = pe.permute(1, 0, 2) # (Shape: 1, max_len, d_model) (S·ª≠a cho batch_first)
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ x: Shape (Batch, Seq_Len, d_model) """
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)

class CVAE_Trans_Decoder(nn.Module):
    def __init__(self, lookback, lookforward, num_features, d_model, n_head, num_enc_layers, num_dec_layers, latent_dim):
        super(CVAE_Trans_Decoder, self).__init__()
        self.lookforward = lookforward; self.d_model = d_model
        self.embed_past = nn.Linear(num_features, d_model)
        self.z_embed = nn.Linear(latent_dim, d_model)
        self.past_feature_embed = nn.Linear(d_model * lookback, d_model)
        self.pos_encoder_past = PositionalEncoding(d_model, max_len=lookback)
        self.pos_encoder_future_query = PositionalEncoding(d_model, max_len=lookforward)
        encoder_layer_past = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head, batch_first=True)
        self.transformer_encoder_past = nn.TransformerEncoder(encoder_layer_past, num_layers=num_enc_layers)
        self.dense_upsample = nn.Linear(d_model + d_model, d_model * lookforward)
        decoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head, batch_first=True)
        self.transformer_decoder = nn.TransformerEncoder(decoder_layer, num_layers=num_dec_layers)
        self.output_layer = nn.Linear(d_model, num_features)
        self.relu = nn.ReLU(); self.sigmoid = nn.Sigmoid()
        
    def forward(self, condition_input, latent_input):
        x_past = self.embed_past(condition_input)
        x_past = self.pos_encoder_past(x_past)
        past_features_seq = self.transformer_encoder_past(x_past)
        past_features_flat = past_features_seq.mean(dim=1) 
        z_features = self.relu(self.z_embed(latent_input)) 
        combined = torch.cat([past_features_flat, z_features], dim=1)
        upsampled_features = self.relu(self.dense_upsample(combined))
        future_query = upsampled_features.view(-1, self.lookforward, self.d_model)
        future_query = self.pos_encoder_future_query(future_query)
        reconstruction_features = self.transformer_decoder(future_query)
        reconstruction = self.sigmoid(self.output_layer(reconstruction_features))
        return reconstruction, None 
# --- (H·∫øt "khu√¥n" CVAE-Transformer) ---

# =========================================================================
# üí° B∆Ø·ªöC 2: "D·ª∞NG" C√ÅC H√ÄM "H·∫¨U C·∫¶N" (LOAD DATA, UNSCALE, L·∫§Y M·ªíI)
# =========================================================================

def load_data_and_set_unscaler(num_features):
    """
    T·∫£i Scaler "V23" (53 m√≥n) v√† Data "th√¥" (53 m√≥n).
    Quan tr·ªçng: T√¨m th√¥ng s·ªë "unscale" c·ªßa H1_Close
    """
    global SCALER_CLOSE_IDX, SCALER_CLOSE_MIN, SCALER_CLOSE_SCALE
    
    logging.info(f"ƒêang t·∫£i Scaler 'g·ªëc' V23 (53 m√≥n): {SCALER_FILENAME_GOC}...")
    try:
        scaler = joblib.load(SCALER_FILENAME_GOC)
        
        # "M√≥c" th√¥ng s·ªë "unscale" c·ªßa H1_Close ra
        try:
            feature_names = list(scaler.feature_names_in_)
        except AttributeError:
             logging.warning("Scaler V23 'c≈©', kh√¥ng c√≥ 'feature_names_in_'. ƒêang 'm√≥c' H1_Close 'th·ªß c√¥ng'...")
             df_temp = pd.read_parquet(MASTER_FILE_PATH_THO)
             feature_names = df_temp.columns.tolist()
             
        SCALER_CLOSE_IDX = feature_names.index('H1_Close')
        SCALER_CLOSE_MIN = scaler.min_[SCALER_CLOSE_IDX]
        SCALER_SCALE_CLOSE = scaler.scale_[SCALER_CLOSE_IDX]
        
        logging.info(f"ƒê√£ 'm√≥c' th√¥ng s·ªë unscale (H1_Close Idx: {SCALER_CLOSE_IDX})")
        
    except FileNotFoundError:
        logging.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y Scaler 'g·ªëc' {SCALER_FILENAME_GOC}")
        logging.error("ƒê·∫°i ca ƒë√£ ch·∫°y 'L√≤' (train_cvae_V11.py) (file 'ƒÉn' 53 m√≥n) [B∆∞·ªõc 1] ch∆∞a?")
        return None
    except Exception as e:
        logging.error(f"L·ªñI: Kh√¥ng t·∫£i/ƒë·ªçc ƒë∆∞·ª£c Scaler 'g·ªëc'. L·ªói: {e}")
        return None

    logging.info(f"ƒêang t·∫£i Data 'th√¥' (53 m√≥n): {MASTER_FILE_PATH_THO}...")
    try:
        df = pd.read_parquet(MASTER_FILE_PATH_THO)
        
        if num_features != df.shape[1]:
            logging.error(f"L·ªñI 'KH·ªöP' N√ÉO: 'L√≤' (V11/V4) 'luy·ªán' {num_features} m√≥n,")
            logging.error(f"nh∆∞ng file 'th√¥' ({MASTER_FILE_PATH_THO}) l·∫°i 'c√≥' {df.shape[1]} m√≥n.")
            return None
        
        df.interpolate(method='time', inplace=True)
        df.fillna(method='ffill', inplace=True)
        df.fillna(method='bfill', inplace=True)
        df.fillna(0, inplace=True) # "Tr√°m" 0 
        
        data_scaled = scaler.transform(df)
        
        logging.info(f"ƒê√£ t·∫£i v√† 'chu·∫©n h√≥a' {data_scaled.shape[0]} n·∫øn H1 'th√¥' (53 m√≥n).")
        return data_scaled
        
    except Exception as e:
        logging.error(f"L·ªñI: Kh√¥ng t·∫£i/x·ª≠ l√Ω ƒë∆∞·ª£c Data 'th√¥'. L·ªói: {e}")
        return None

def get_real_sample(data_scaled, lookback, sample_idx=-1000):
    """
    "M√≥c" 1 m·∫©u "qu√° kh·ª©" (lookback) l√†m "m·ªìi"
    """
    try:
        real_sample_np = data_scaled[sample_idx - lookback : sample_idx]
        real_future_np = data_scaled[sample_idx : sample_idx + LOOKFORWARD_CVAE]
        
        if real_sample_np.shape[0] != lookback or real_future_np.shape[0] != LOOKFORWARD_CVAE:
            logging.error("L·ªñI 'M√ìC M·ªíI': Kh√¥ng ƒë·ªß data ƒë·ªÉ 'm√≥c' (sample_idx qu√° g·∫ßn cu·ªëi).")
            return None, None
            
        real_sample_gpu = torch.tensor(real_sample_np, dtype=torch.float32).unsqueeze(0).to(device)
        
        return real_sample_gpu, real_future_np
        
    except Exception as e:
        logging.error(f"L·ªñI 'M√ìC M·ªíI': {e}")
        return None, None

def unscale_h1_close(scaled_data_np):
    """
    H√†m "th·∫ßn th√°nh": "Unscale" ch·ªâ ri√™ng c·ªôt H1_Close
    """
    try:
        if scaled_data_np.ndim == 3:
            scaled_close = scaled_data_np[0, :, SCALER_CLOSE_IDX]
        else:
            scaled_close = scaled_data_np[:, SCALER_CLOSE_IDX]
        unscaled_close = (scaled_close * SCALER_SCALE_CLOSE) + SCALER_CLOSE_MIN
        return unscaled_close
    except Exception as e:
        logging.error(f"L·ªói 'Unscale': {e}")
        return np.zeros(scaled_data_np.shape[1]) 

# =========================================================================
# üí° B∆Ø·ªöC 3: "D·ª∞NG" 3 "PH√íNG TRI·ªÇN L√ÉM" (CVAE, TIMEGAN, TCVAE)
# =========================================================================

def plot_cvae_scenarios(decoder_model, real_past_gpu, real_future_np, lookback, lookforward, num_scenarios, model_name, file_name_suffix):
    """
    H√†m "V·∫Ω" CHUNG (D√πng cho CVAE-LSTM v√† CVAE-Transformer)
    """
    logging.info(f"--- ƒêang 'v·∫Ω' {num_scenarios} k·ªãch b·∫£n {model_name} (LB={lookback}) ---")
    
    decoder_model.eval()
    
    past_unscaled = unscale_h1_close(real_past_gpu.cpu().numpy())
    future_unscaled_real = unscale_h1_close(real_future_np)
    
    x_past = np.arange(lookback)
    x_future = np.arange(lookback, lookback + lookforward)
    
    plt.figure(figsize=(20, 8))
    plt.plot(x_past, past_unscaled, 'r-', linewidth=3, label=f"Qu√° Kh·ª© (M·ªìi {lookback} n·∫øn)")
    plt.plot(x_future, future_unscaled_real, 'g--', linewidth=3, label=f"T∆∞∆°ng Lai (Th·∫≠t {lookforward} n·∫øn)")
    
    for i in range(num_scenarios):
        with torch.no_grad():
            z_noise = torch.randn(1, LATENT_DIM_CVAE).to(device)
            future_fake_scaled, _ = decoder_model(real_past_gpu, z_noise)
            future_unscaled_fake = unscale_h1_close(future_fake_scaled.cpu().numpy())
            plt.plot(x_future, future_unscaled_fake, 'b-', alpha=0.3, label=f'K·ªãch b·∫£n {model_name} {i+1}' if i < 1 else None)
            
    plt.title(f"PH√íNG TRI·ªÇN L√ÉM {model_name} (53 M√≥n): {num_scenarios} K·ªãch B·∫£n T∆∞∆°ng Lai (LB={lookback})", fontsize=16)
    plt.ylabel("Gi√° H1_Close (USDT)", fontsize=12)
    plt.xlabel("N·∫øn H1", fontsize=12)
    plt.legend(loc='upper left')
    plt.grid(True)
    
    save_path = os.path.join(CHART_SAVE_DIR, f"{file_name_suffix}_Scenarios_LB{lookback}.png")
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    logging.info(f"ƒê√£ 'v·∫Ω' v√† l∆∞u ·∫£nh {model_name} -> {save_path}")
    plt.close()

def plot_timegan_scenarios(g_model, r_model, lookback, hidden_dim, num_scenarios=10):
    """
    "V·∫Ω" 10 k·ªãch b·∫£n "Qu√° Kh·ª©" (t·ª± b·ªãa) c·ªßa Th·∫ßy TimeGAN (V4 - 53 m√≥n)
    """
    logging.info(f"--- ƒêang 'v·∫Ω' {num_scenarios} k·ªãch b·∫£n TimeGAN V4 (LB={lookback}) ---")
    
    g_model.eval()
    r_model.eval()
    
    x_axis = np.arange(lookback)
    
    plt.figure(figsize=(20, 8))
    
    for i in range(num_scenarios):
        with torch.no_grad():
            z_noise = torch.randn(1, lookback, hidden_dim).to(device)
            h_fake_scaled = g_model(z_noise)
            x_fake_scaled = r_model(h_fake_scaled)
            x_unscaled_fake = unscale_h1_close(x_fake_scaled.cpu().numpy())
            plt.plot(x_axis, x_unscaled_fake, 'g-', alpha=0.4, label=f'K·ªãch b·∫£n TimeGAN {i+1}' if i < 1 else None)

    plt.title(f"PH√íNG TRI·ªÇN L√ÉM TIMEGAN V4 (53 M√≥n): {num_scenarios} K·ªãch B·∫£n 'T·ª± B·ªãa' (LB={lookback})", fontsize=16)
    plt.ylabel("Gi√° H1_Close (USDT)", fontsize=12)
    plt.xlabel(f"{lookback} N·∫øn H1 (Gi·∫£)", fontsize=12)
    plt.legend(loc='upper left')
    plt.grid(True)
    
    save_path = os.path.join(CHART_SAVE_DIR, f"TimeGAN_V4_Scenarios_LB{lookback}.png")
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    logging.info(f"ƒê√£ 'v·∫Ω' v√† l∆∞u ·∫£nh TimeGAN V4 -> {save_path}")
    plt.close()


# =========================================================================
# üí° B∆Ø·ªöC 4: "M·ªû C·ª¨A" PH√íNG TRI·ªÇN L√ÉM (C·∫¢ 3 N√ÉO)
# =========================================================================
if __name__ == "__main__":
    logging.info(f"=== B·∫ÆT ƒê·∫¶U 'M·ªû C·ª¨A' PH√íNG TRI·ªÇN L√ÉM (C·∫¢ 3 N√ÉO V·∫º) (53 M√≥n) ===")
    
    # 1. "N·∫†P TH·ª®C ƒÇN" (53 M√≥n) V√Ä "SCALER V23"
    data_scaled = load_data_and_set_unscaler(num_features=NUM_FEATURES_GOC)
    
    if data_scaled is None:
        logging.error("D·ª´ng 'Ph√≤ng Tri·ªÉn L√£m' v√¨ kh√¥ng c√≥ 'th·ª©c ƒÉn' ho·∫∑c 'scaler'.")
        sys.exit(1)
        
    # 2. "M√ìC M·ªíI" (L·∫•y 50 n·∫øn "th·∫≠t" g·∫ßn cu·ªëi l√†m "m·ªìi")
    # (D√πng chung "m·ªìi" (sample_idx=-100) cho CVAE-LSTM v√† CVAE-Transformer)
    real_past_gpu, real_future_np = get_real_sample(data_scaled, LOOKBACK, sample_idx=-100)
    
    if real_past_gpu is None:
        logging.error("D·ª´ng 'Ph√≤ng Tri·ªÉn L√£m' v√¨ kh√¥ng 'm√≥c m·ªìi' ƒë∆∞·ª£c.")
        sys.exit(1)

    # 3. "TRI·ªÇN L√ÉM" TRANH C·ª¶A TH·∫¶Y 1: CVAE-LSTM (V11)
    try:
        logging.info("--- ƒêang 'h·ªìi sinh' N√ÉO 1: CVAE-LSTM (V11) (53 M√≥n) ---")
        cvae_model_path = os.path.join(DIR_MODELS, CVAE_LSTM_MODEL_FILE.format(lb=LOOKBACK, lf=LOOKFORWARD_CVAE))
        
        cvae_decoder_lstm = CVAE_LSTM_Decoder(LOOKBACK, LOOKFORWARD_CVAE, NUM_FEATURES_GOC, LATENT_DIM_CVAE).to(device)
        cvae_decoder_lstm.load_state_dict(torch.load(cvae_model_path, map_location=device))
        
        logging.info(f"ƒê√£ 'h·ªìi sinh' n√£o CVAE-LSTM V11 t·ª´: {cvae_model_path}")
        
        # "V·∫Ω" (D√πng h√†m "V·∫Ω" chung)
        plot_cvae_scenarios(
            cvae_decoder_lstm, real_past_gpu, real_future_np, 
            LOOKBACK, LOOKFORWARD_CVAE, 
            num_scenarios=10, 
            model_name="CVAE-LSTM (V11)",
            file_name_suffix="CVAE_LSTM_V11"
        )
        
    except FileNotFoundError:
        logging.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y 'N√£o CVAE V11' t·∫°i: {cvae_model_path}")
        logging.error("ƒê·∫°i ca ƒë√£ ch·∫°y 'L√≤' (train_cvae_V11.py) (file 'ƒÉn' 53 m√≥n) [B∆∞·ªõc 1] ch∆∞a?")
    except Exception as e:
        logging.error(f"L·ªñI 'H·ªíI SINH' N√ÉO CVAE V11: {e}")

    # 4. "TRI·ªÇN L√ÉM" TRANH C·ª¶A TH·∫¶Y 2: TIMEGAN-GRU (V4)
    try:
        logging.info("--- ƒêang 'h·ªìi sinh' N√ÉO 2: TIMEGAN-GRU (V4) (53 M√≥n) ---")
        timegan_model_path = os.path.join(DIR_MODELS, TIMEGAN_GRU_MODEL_FILE.format(lb=LOOKBACK))
        
        timegan_G = TimeGAN_GRU_Generator(HIDDEN_DIM_TIMEGAN).to(device)
        timegan_R = TimeGAN_GRU_Recovery(NUM_FEATURES_GOC, HIDDEN_DIM_TIMEGAN).to(device)
        
        checkpoint = torch.load(timegan_model_path, map_location=device)
        timegan_G.load_state_dict(checkpoint['G_state_dict'])
        timegan_R.load_state_dict(checkpoint['R_state_dict'])
        
        logging.info(f"ƒê√£ 'h·ªìi sinh' n√£o TimeGAN V4 (G v√† R) t·ª´: {timegan_model_path}")
        
        # "V·∫Ω"
        plot_timegan_scenarios(timegan_G, timegan_R, LOOKBACK, HIDDEN_DIM_TIMEGAN, num_scenarios=10)

    except FileNotFoundError:
        logging.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y 'N√£o TimeGAN V4' t·∫°i: {timegan_model_path}")
        logging.error("ƒê·∫°i ca ƒë√£ ch·∫°y 'L√≤' (train_timegan_V4.py) (file 'ƒÉn' 53 m√≥n) [B∆∞·ªõc 2] ch∆∞a?")
    except Exception as e:
        logging.error(f"L·ªñI 'H·ªíI SINH' N√ÉO TIMEGAN V4: {e}")

    # 5. (M·ªöI) "TRI·ªÇN L√ÉM" TRANH C·ª¶A TH·∫¶Y 3: CVAE-TRANSFORMER (TCVAE V1)
    try:
        logging.info("--- ƒêang 'h·ªìi sinh' N√ÉO 3: CVAE-Transformer (V13) (53 M√≥n) ---")
        cvae_trans_model_path = os.path.join(DIR_MODELS, CVAE_TRANS_MODEL_FILE.format(lb=LOOKBACK, lf=LOOKFORWARD_CVAE))
        
        cvae_decoder_trans = CVAE_Trans_Decoder(
            LOOKBACK, LOOKFORWARD_CVAE, NUM_FEATURES_GOC, 
            D_MODEL, N_HEAD, NUM_ENC_LAYERS, NUM_DEC_LAYERS, LATENT_DIM_CVAE
        ).to(device)
        
        cvae_decoder_trans.load_state_dict(torch.load(cvae_trans_model_path, map_location=device))
        
        logging.info(f"ƒê√£ 'h·ªìi sinh' n√£o CVAE-Transformer V13 t·ª´: {cvae_trans_model_path}")
        
        # "V·∫Ω" (D√πng h√†m "V·∫Ω" chung)
        plot_cvae_scenarios(
            cvae_decoder_trans, real_past_gpu, real_future_np, 
            LOOKBACK, LOOKFORWARD_CVAE, 
            num_scenarios=10, 
            model_name="CVAE-Transformer (V13)",
            file_name_suffix="CVAE_TRANS_V13"
        )
        
    except FileNotFoundError:
        logging.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y 'N√£o TCVAE V13' t·∫°i: {cvae_trans_model_path}")
        logging.error("ƒê·∫°i ca ƒë√£ ch·∫°y 'L√≤' (04_train_transformer_cvae_V1...) (file 'ƒÉn' 53 m√≥n) [B∆∞·ªõc 3] ch∆∞a?")
    except Exception as e:
        logging.error(f"L·ªñI 'H·ªíI SINH' N√ÉO TCVAE V13: {e}")

    logging.info(f"\n{'='*70}\n === HO√ÄN T·∫§T! ƒê√É 'V·∫º' XONG TRANH (C·∫¢ 3 N√ÉO)! ===\n{'='*70}")
    logging.info(f"ƒê·∫°i ca v√†o th∆∞ m·ª•c '{CHART_SAVE_DIR}' ƒë·ªÉ 'th∆∞·ªüng th·ª©c' 3 b·ª©c ·∫£nh PNG nh√©!")