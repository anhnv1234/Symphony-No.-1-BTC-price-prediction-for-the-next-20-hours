import pandas as pd
import numpy as np
import os
import logging
import joblib
import argparse
import schedule
import time
import sys # (Th√™m sys ƒë·ªÉ "b·ªãt mi·ªáng" stdout)
import matplotlib
matplotlib.use('Agg') # <<<--- QUAN TR·ªåNG: Ch·∫ø ƒë·ªô "v·∫Ω" kh√¥ng c·∫ßn m√†n h√¨nh
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.gridspec import GridSpec # (S·ª¨A V53: D√πng "l∆∞·ªõi" "x·ªãn")
import torch
import torch.nn as nn
import math # (C·∫ßn cho "N√£o" Transformer)
import warnings

# --- "ƒê·ªò" H√ÄNG KH·ª¶NG "SOI" QU√Å KH·ª® (stumpy) ---
try:
    import stumpy
except ImportError:
    print("L·ªñI: Thi·∫øu 'linh ki·ªán' stumpy (H√†ng Kh·ªßng).")
    print("ƒê·∫°i ca vui l√≤ng ch·∫°y: pip install stumpy")
    exit()

# --- (FIX "N·∫æN") "ƒê·ªò" H√ÄNG V·∫º "N·∫æN" (mplfinance) ---
try:
    import mplfinance as mpf
except ImportError:
    print("L·ªñI: Thi·∫øu 'linh ki·ªán' mplfinance (H√†ng \"N·∫øn\" X·ªãn).")
    print("ƒê·∫°i ca vui l√≤ng ch·∫°y: pip install mplfinance")
    exit()

# --- S·ª¨A L·ªñI IMPORT (THEO FILE C·ª¶A ƒê·∫†I CA) ---
try:
    # (D√πng "L√≤" Data Service V23 (file 11:24 AM) c·ªßa ƒë·∫°i ca)
    from data_service import MasterDataServiceV23
except ImportError:
    print("L·ªñI: Kh√¥ng t√¨m th·∫•y file 'data_service.py' (ch·ª©a MasterDataServiceV23).")
    print("Vui l√≤ng ƒë·∫£m b·∫£o file ƒë√≥ ·ªü c√πng th∆∞ m·ª•c.")
    exit()
# ----------------------------------------

# --- (M√ìN 1) "B·ªäT MI·ªÜNG" TO√ÄN B·ªò C·∫¢NH B√ÅO "R√ÅC" ---
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# --- C·∫•u h√¨nh logging (ƒê√É S·ª¨A L·ªñI LOGGER) ---
# (S·ª≠a l·ªói UTF-8 cho Windows)
try:
    sys.stdout.reconfigure(encoding='utf-8')
except AttributeError:
    pass
    
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - [LiveBot_V53_ALL] - %(message)s',
                    handlers=[
                        logging.FileHandler("log_05_live_bot_V53_ALL.log", mode='w', encoding='utf-8'), 
                        logging.StreamHandler(sys.stdout) # (V·∫´n "phun" ra console)
                    ])

# --- H·∫±ng s·ªë "V·∫Ω" V53 ---
CHART_FILENAME = 'live_prediction_chart_V53_ALL.png' 
LOOKFORWARD = 20 # (M·∫∑c ƒë·ªãnh c·ªßa CVAE/TCVAE)

# --- (S·ª¨A V53i) "SI·∫æT C·ªî" TIMEGAN (0.05 = 5% ƒë·ªô bi·∫øn ƒë·ªông) ---
TIMEGAN_DAMPING_FACTOR = 0.05 

# --- (S·ª¨A V53) C·∫§U H√åNH "B·ªò N√ÉO" (Ph·∫£i "kh·ªõp" 100% l√∫c "luy·ªán") ---
# (CVAE-LSTM & TimeGAN-GRU)
LATENT_DIM_CVAE = 32 
HIDDEN_DIM_TIMEGAN = 24 
NUM_FEATURES_GOC = 53 # (S·ªë m√≥n "g·ªëc")
# (TCVAE)
D_MODEL = 64      
N_HEAD = 4        
NUM_ENC_LAYERS = 2 
NUM_DEC_LAYERS = 2 

# --- (S·ª¨A V53) BI·∫æN TO√ÄN C·ª§C (Load 6 N√£o + 1 Scaler) ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
SCALER_V23 = None # (Scaler "ch√≠ m·∫°ng" 53 m√≥n)
MASTER_DATA_COLUMNS = None 
H1_COL_INDICES = {} # (V·ªã tr√≠ "m√≥c" H1_Close)
# (Bi·∫øn "to√†n c·ª•c" ƒë·ªÉ "l·∫•y" th√¥ng s·ªë "unscale" H1_Close - S·∫Ω ƒë∆∞·ª£c "m√≥c" (get) l√∫c load_all_brains)
SCALER_CLOSE_IDX = -1
SCALER_CLOSE_MIN = 0.0
SCALER_SCALE_CLOSE = 1.0

# (6 "N√£o")
CVAE_LSTM_50 = None
CVAE_LSTM_168 = None
TIMEGAN_G_50 = None
TIMEGAN_R_50 = None
TIMEGAN_G_168 = None
TIMEGAN_R_168 = None
TCVAE_50 = None
TCVAE_168 = None

# =========================================================================
# üí° B∆Ø·ªöC 1: "B√ä" (COPY) C√ÅC "KHU√îN N√ÉO" (CLASSES) T·ª™ 3 FILE "L√í"
# =========================================================================

# --- "KHU√îN N√ÉO" 1: CVAE-LSTM (T·ª´ file train_cvae_V11.py) ---
def sampling(args):
    z_mean, z_log_var = args
    batch = z_mean.shape[0]; dim = z_mean.shape[1]
    epsilon = torch.randn(size=(batch, dim)).to(device)
    return z_mean + torch.exp(0.5 * z_log_var) * epsilon

class CVAE_LSTM_Decoder(nn.Module):
    def __init__(self, lookback, lookforward, num_features, latent_dim, num_heads=4):
        super(CVAE_LSTM_Decoder, self).__init__()
        self.lookforward = lookforward; self.num_features = num_features; self.latent_dim = latent_dim 
        self.lstm_past_1 = nn.LSTM(num_features, 64, batch_first=True, num_layers=1)
        self.lstm_past_2 = nn.LSTM(64, 64, batch_first=True, num_layers=1)
        self.attention = nn.MultiheadAttention(embed_dim=64, num_heads=num_heads, batch_first=True) 
        self.z_to_query_upscaler = nn.Linear(latent_dim, 64)
        self.dense_combine = nn.Linear(latent_dim + 64 + 64, 64 * lookforward)
        self.lstm_gen = nn.LSTM(64, 128, batch_first=True, num_layers=1)
        self.time_dist_dense = nn.Linear(128, num_features)
        self.relu = nn.ReLU(); self.sigmoid = nn.Sigmoid()
        
    def forward(self, condition_input, latent_input):
        h_past_seq, (last_hidden_past, _) = self.lstm_past_1(condition_input)
        _, (last_hidden_past_2, _) = self.lstm_past_2(h_past_seq)
        cond_features = self.relu(last_hidden_past_2.squeeze(0)) 
        z_query_upscaled = self.relu(self.z_to_query_upscaler(latent_input)) 
        query_vector = self.relu(cond_features + z_query_upscaled) 
        query_vector = query_vector.unsqueeze(1) 
        context_vector, attn_weights = self.attention(
            query=query_vector, key=h_past_seq, value=h_past_seq
        )
        context_vector = context_vector.squeeze(1) 
        combined = torch.cat([latent_input, cond_features, context_vector], dim=1)
        x = self.relu(self.dense_combine(combined))
        x = x.view(-1, self.lookforward, 64)
        x, _ = self.lstm_gen(x)
        x = self.time_dist_dense(x)
        reconstruction = self.sigmoid(x)
        return reconstruction, attn_weights
# --- (H·∫øt "khu√¥n" CVAE-LSTM) ---

# --- "KHU√îN N√ÉO" 2: TIMEGAN-GRU (T·ª´ file train_timegan_V4.py) ---
class BaseGRU(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim, num_layers=2):
        super().__init__()
        self.rnn = nn.GRU(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)
        self.output_layer = nn.Linear(hidden_dim, output_dim)
    def forward(self, x):
        output, _ = self.rnn(x); return self.output_layer(output)

class TimeGAN_GRU_Generator(BaseGRU):
    def __init__(self, hidden_dim):
        super().__init__(hidden_dim, hidden_dim, hidden_dim)

class TimeGAN_GRU_Recovery(BaseGRU):
    def __init__(self, num_features, hidden_dim):
        super().__init__(hidden_dim, num_features, hidden_dim)
# --- (H·∫øt "khu√¥n" TimeGAN-GRU) ---

# --- "KHU√îN N√ÉO" 3: CVAE-Transformer (T·ª´ file 04_train_transformer_cvae_V1.py) ---
class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        pe = pe.permute(1, 0, 2) # (Shape: 1, max_len, d_model) (S·ª≠a cho batch_first)
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ x: Shape (Batch, Seq_Len, d_model) """
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)

class CVAE_Trans_Decoder(nn.Module):
    def __init__(self, lookback, lookforward, num_features, d_model, n_head, num_enc_layers, num_dec_layers, latent_dim):
        super(CVAE_Trans_Decoder, self).__init__()
        self.lookforward = lookforward; self.d_model = d_model
        self.embed_past = nn.Linear(num_features, d_model)
        self.z_embed = nn.Linear(latent_dim, d_model)
        self.past_feature_embed = nn.Linear(d_model * lookback, d_model)
        self.pos_encoder_past = PositionalEncoding(d_model, max_len=lookback)
        self.pos_encoder_future_query = PositionalEncoding(d_model, max_len=lookforward)
        encoder_layer_past = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head, batch_first=True)
        self.transformer_encoder_past = nn.TransformerEncoder(encoder_layer_past, num_layers=num_enc_layers)
        self.dense_upsample = nn.Linear(d_model + d_model, d_model * lookforward)
        decoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head, batch_first=True)
        self.transformer_decoder = nn.TransformerEncoder(decoder_layer, num_layers=num_dec_layers)
        self.output_layer = nn.Linear(d_model, num_features)
        self.relu = nn.ReLU(); self.sigmoid = nn.Sigmoid()
        
    def forward(self, condition_input, latent_input):
        x_past = self.embed_past(condition_input)
        x_past = self.pos_encoder_past(x_past)
        past_features_seq = self.transformer_encoder_past(x_past)
        past_features_flat = past_features_seq.mean(dim=1) 
        z_features = self.relu(self.z_embed(latent_input)) 
        combined = torch.cat([past_features_flat, z_features], dim=1)
        upsampled_features = self.relu(self.dense_upsample(combined))
        future_query = upsampled_features.view(-1, self.lookforward, self.d_model)
        future_query = self.pos_encoder_future_query(future_query)
        reconstruction_features = self.transformer_decoder(future_query)
        reconstruction = self.sigmoid(self.output_layer(reconstruction_features))
        return reconstruction, None 
# --- (H·∫øt "khu√¥n" CVAE-Transformer) ---


# =========================================================================
# üí° B∆Ø·ªöC 2: "H·ªíI SINH" 6 "N√ÉO" V√Ä 1 "SCALER"
# =========================================================================

def load_all_brains():
    """Load 6 file "N√£o B·ªô" (50, 168) v√† 1 Scaler (V23) v√†o RAM."""
    global SCALER_V23
    global CVAE_LSTM_50, CVAE_LSTM_168
    global TIMEGAN_G_50, TIMEGAN_R_50, TIMEGAN_G_168, TIMEGAN_R_168
    global TCVAE_50, TCVAE_168
    global MASTER_DATA_COLUMNS, H1_COL_INDICES
    global SCALER_CLOSE_IDX, SCALER_CLOSE_MIN, SCALER_SCALE_CLOSE # (Th√™m)
    
    logging.info(f"ƒêang n·∫°p 'H·ªôi ƒê·ªìng N√£o B·ªô' (6 N√£o 53 M√≥n) v√† Scaler V23...")
    
    # --- T√™n file "ch√≠ m·∫°ng" ---
    scaler_file = os.path.join('01_Processed_Data', 'cvae_scaler_V23.gz')
    
    cvae_lstm_50_file = os.path.join('03_Models', f'cvae_decoder_V11_100PCT_50_{LOOKFORWARD}.pth')
    cvae_lstm_168_file = os.path.join('03_Models', f'cvae_decoder_V11_100PCT_168_{LOOKFORWARD}.pth')
    
    timegan_50_file = os.path.join('03_Models', 'advanced_tsgan_model_50_final.pth')
    timegan_168_file = os.path.join('03_Models', 'advanced_tsgan_model_168_final.pth')
    
    tcvae_50_file = os.path.join('03_Models', f'transformer_cvae_decoder_V13_50_{LOOKFORWARD}_best.pth')
    # (ƒê·∫°i ca 's·ª≠a' t√™n file n√†y (n·∫øu c·∫ßn) cho 'kh·ªõp' v·ªõi file 'resume' ƒë√£ 'rename' nh√©)
    tcvae_168_file = os.path.join('03_Models', 'transformer_cvae_model_V13_168_resume.pth')
    
    try:
        # 1. "H·ªìi sinh" Scaler V23 (53 m√≥n)
        SCALER_V23 = joblib.load(scaler_file)
        logging.info(f"N·∫°p Scaler V23 (53 m√≥n) th√†nh c√¥ng.")
        
        # "M√≥c" (Get) 53 "t√™n m√≥n" (features) v√† "v·ªã tr√≠" (index) H1_Close
        try:
            MASTER_DATA_COLUMNS = list(SCALER_V23.feature_names_in_)
        except AttributeError:
             logging.warning("Scaler V23 'c≈©', kh√¥ng c√≥ 'feature_names_in_'. ƒêang 'm√≥c' H1_Close 'th·ªß c√¥ng'...")
             df_temp = pd.read_parquet(os.path.join('02_Master_Data', 'btcusdt_master_data.parquet'))
             MASTER_DATA_COLUMNS = df_temp.columns.tolist()
             
        H1_COL_INDICES['Close'] = MASTER_DATA_COLUMNS.index('H1_Close')
        H1_COL_INDICES['Open'] = MASTER_DATA_COLUMNS.index('H1_Open')
        H1_COL_INDICES['High'] = MASTER_DATA_COLUMNS.index('H1_High')
        H1_COL_INDICES['Low'] = MASTER_DATA_COLUMNS.index('H1_Low')
        H1_COL_INDICES['Volume'] = MASTER_DATA_COLUMNS.index('H1_Volume') 
        logging.info(f"ƒê√£ 'm√≥c' v·ªã tr√≠ 5 c·ªôt H1 (Close={H1_COL_INDICES['Close']})")
        
        # (S·ª¨A V53) "M√≥c" (Get) th√¥ng s·ªë "unscale" H1_Close
        SCALER_CLOSE_IDX = H1_COL_INDICES['Close']
        SCALER_CLOSE_MIN = SCALER_V23.min_[SCALER_CLOSE_IDX]
        SCALER_SCALE_CLOSE = SCALER_V23.scale_[SCALER_CLOSE_IDX]
        logging.info(f"ƒê√£ 'm√≥c' th√¥ng s·ªë Unscale (Min: {SCALER_CLOSE_MIN}, Scale: {SCALER_SCALE_CLOSE})")
        
        # 2. "H·ªìi sinh" "N√£o" CVAE-LSTM (V11)
        CVAE_LSTM_50 = CVAE_LSTM_Decoder(50, LOOKFORWARD, NUM_FEATURES_GOC, LATENT_DIM_CVAE).to(device)
        CVAE_LSTM_50.load_state_dict(torch.load(cvae_lstm_50_file, map_location=device))
        CVAE_LSTM_50.eval()
        CVAE_LSTM_168 = CVAE_LSTM_Decoder(168, LOOKFORWARD, NUM_FEATURES_GOC, LATENT_DIM_CVAE).to(device)
        CVAE_LSTM_168.load_state_dict(torch.load(cvae_lstm_168_file, map_location=device))
        CVAE_LSTM_168.eval()
        logging.info(f"N·∫°p 'N√£o 1' (CVAE-LSTM V11 x2) th√†nh c√¥ng.")
        
        # 3. "H·ªìi sinh" "N√£o" TimeGAN (V4)
        checkpoint_50 = torch.load(timegan_50_file, map_location=device)
        TIMEGAN_G_50 = TimeGAN_GRU_Generator(HIDDEN_DIM_TIMEGAN).to(device)
        TIMEGAN_R_50 = TimeGAN_GRU_Recovery(NUM_FEATURES_GOC, HIDDEN_DIM_TIMEGAN).to(device)
        TIMEGAN_G_50.load_state_dict(checkpoint_50['G_state_dict']); TIMEGAN_G_50.eval()
        TIMEGAN_R_50.load_state_dict(checkpoint_50['R_state_dict']); TIMEGAN_R_50.eval()
        
        checkpoint_168 = torch.load(timegan_168_file, map_location=device)
        TIMEGAN_G_168 = TimeGAN_GRU_Generator(HIDDEN_DIM_TIMEGAN).to(device)
        TIMEGAN_R_168 = TimeGAN_GRU_Recovery(NUM_FEATURES_GOC, HIDDEN_DIM_TIMEGAN).to(device)
        TIMEGAN_G_168.load_state_dict(checkpoint_168['G_state_dict']); TIMEGAN_G_168.eval()
        TIMEGAN_R_168.load_state_dict(checkpoint_168['R_state_dict']); TIMEGAN_R_168.eval()
        logging.info(f"N·∫°p 'N√£o 2' (TimeGAN-GRU V4 x2) th√†nh c√¥ng.")
        
        # 4. "H·ªìi sinh" "N√£o" TCVAE (V1)
        
        # --- (N√ÉO 50 - Gi·ªØ nguy√™n - File "plot" ch·∫°y OK) ---
        TCVAE_50 = CVAE_Trans_Decoder(50, LOOKFORWARD, NUM_FEATURES_GOC, D_MODEL, N_HEAD, NUM_ENC_LAYERS, NUM_DEC_LAYERS, LATENT_DIM_CVAE).to(device)
        TCVAE_50.load_state_dict(torch.load(tcvae_50_file, map_location=device))
        TCVAE_50.eval()
        
        # --- (N√ÉO 168 - *S·ª¨A L·ªñI "V·∫†CH BALO"*) ---
        TCVAE_168 = CVAE_Trans_Decoder(168, LOOKFORWARD, NUM_FEATURES_GOC, D_MODEL, N_HEAD, NUM_ENC_LAYERS, NUM_DEC_LAYERS, LATENT_DIM_CVAE).to(device)
        
        logging.info("ƒêang 'v·∫°ch Balo' (unpack) N√£o TCVAE 168 (do l·ªói state_dict)...")
        checkpoint_168 = torch.load(tcvae_168_file, map_location=device)
        TCVAE_168.load_state_dict(checkpoint_168['decoder_state_dict']) 
        
        TCVAE_168.eval()
        
        logging.info(f"N·∫°p 'N√£o 3' (TCVAE V1 x2) th√†nh c√¥ng.")
        
        return True
        
    except Exception as e:
        logging.critical(f"L·ªñI CH√ç M·∫†NG: Kh√¥ng n·∫°p ƒë∆∞·ª£c 'N√£o B·ªô'! L·ªói: {e}")
        logging.critical("ƒê·∫°i ca ƒë√£ ch·∫°y 'ƒê√öNG TH√ö T·ª∞' 3 L√≤ (V11 -> V4 -> TCVAE V1) ch∆∞a?")
        return False

# =========================================================================
# üí° B∆Ø·ªöC 3: "H·∫¨U C·∫¶N" (L·∫§Y M·ªíI, "SOI" QU√Å KH·ª®, V·∫º)
# =========================================================================

# --- (*B·∫ÆT ƒê·∫¶U S·ª¨A L·ªñI V53d: "CAMERA AN NINH"*) ---
def get_current_window_scaled_from_df(df_master_full, lookback):
    """
    "C·∫Øt" (slice) "m·ªìi" (past window) 50 ho·∫∑c 168 n·∫øn (ƒë√£ "chu·∫©n h√≥a")
    L∆ØU √ù: H√†m n√†y "gi·∫£ ƒë·ªãnh" df_master_full ƒê√É ƒê∆Ø·ª¢C "V·ªÜ SINH" S·∫†CH S·∫º
    """
    past_window_df = df_master_full.iloc[-lookback:]
            
    if len(past_window_df) < lookback:
        logging.warning(f"Kh√¥ng ƒë·ªß {lookback} n·∫øn. Ch·ªù th√™m...")
        return None, None
        
    # --- "CAMERA" S·ªê 1: "M·ªìi" "Th√¥" (tr∆∞·ªõc khi "√©p d·∫ªo") ---
    # (T·∫ÆT "CAMERA" 1)
    nan_count = past_window_df.isna().sum().sum()
    if nan_count > 0:
        logging.error(f"[V53p] L·ªñI NGHI√äM TR·ªåNG: 'M·ªíI' 'TH√î' V·∫™N C√íN {nan_count} 'NaN'!!!")
    # --- (H·∫øt "Camera" 1) ---

    # L·∫•y "th·ª©c ƒÉn" (53 m√≥n)
    window_scaled = SCALER_V23.transform(past_window_df)
    
    # --- "CAMERA" S·ªê 2: "M·ªìi" "√âp D·∫ªo" (sau khi "scale") ---
    # (T·∫ÆT "CAMERA" 2)
    scaled_close_col = window_scaled[:, SCALER_CLOSE_IDX]
    nan_count_scaled = np.isnan(window_scaled).sum()
    if nan_count_scaled > 0:
         logging.error(f"[V53p] L·ªñI NGHI√äM TR·ªåNG: 'M·ªíI' '√âP D·∫∫O' B·ªä 'NaN' ({nan_count_scaled} 'l·ªói')!!!")
    elif np.all(scaled_close_col == 0):
        logging.warning("[V53p] C·∫¢NH B√ÅO: 'M·ªìi' H1_Close '√âP D·∫∫O' 'to√†n' 's·ªë' 0! (ƒê√¢y c√≥ th·ªÉ l√† 'th·ªß ph·∫°m')")
    # --- (H·∫øt "Camera" 2) ---

    window_scaled_gpu = torch.tensor(window_scaled, dtype=torch.float32).unsqueeze(0).to(device) 
    
    # L·∫•y "h√†ng" (OHLCV) "th√¥" (ƒë·ªÉ "v·∫Ω" n·∫øn)
    past_ohlcv = past_window_df[['H1_Open', 'H1_High', 'H1_Low', 'H1_Close', 'H1_Volume']]
    
    return window_scaled_gpu, past_ohlcv
# --- (*K·∫æT TH√öC S·ª¨A L·ªñI V53d*) ---

def get_mean_scenario(decoder_model, window_scaled_gpu):
    """
    "V·∫Ω" 1 k·ªãch b·∫£n "D·ª± Ki·∫øn" (Mean Z-vector)
    """
    with torch.no_grad():
        z_noise = torch.zeros(1, LATENT_DIM_CVAE).to(device)
        future_fake_scaled, _ = decoder_model(window_scaled_gpu, z_noise)
        return future_fake_scaled.cpu().numpy()

# --- (S·ª¨A L·ªñI V53b: "V√äNH CH√ÇN" TIMEGAN) ---
def get_timegan_scenario(g_model, r_model, lookback):
    """
    "V·∫Ω" 1 k·ªãch b·∫£n "T·ª± B·ªãa" c·ªßa TimeGAN
    """
    with torch.no_grad():
        # (S·ª¨A L·ªñI: Ph·∫£i "b·ªãa" 20 (LOOKFORWARD) n·∫øn, kh√¥ng ph·∫£i "lookback" n·∫øn!)
        z_noise = torch.zeros(1, LOOKFORWARD, HIDDEN_DIM_TIMEGAN).to(device) # (Z=0)
        h_fake_scaled = g_model(z_noise)
        x_fake_scaled = r_model(h_fake_scaled)
        return x_fake_scaled.cpu().numpy()

# --- (S·ª¨A L·ªñI V53e: "C√îNG TH·ª®C UN SCALE") ---
def unscale_h1_close(scaled_data_np):
    """
    H√†m "th·∫ßn th√°nh": "Unscale" ch·ªâ ri√™ng c·ªôt H1_Close
    """
    global SCALER_CLOSE_IDX, SCALER_CLOSE_MIN, SCALER_SCALE_CLOSE
    
    try:
        if scaled_data_np.ndim == 3:
            scaled_close = scaled_data_np[0, :, SCALER_CLOSE_IDX]
        else:
            scaled_close = scaled_data_np[:, SCALER_CLOSE_IDX]
            
        # --- (S·ª¨A V53e: S·ª¨A C√îNG TH·ª®C TO√ÅN) ---
        # (C√¥ng th·ª©c C≈®: (scaled * Scale) + Min)
        unscaled_close = (scaled_close - SCALER_CLOSE_MIN) / SCALER_SCALE_CLOSE
        # --- (H·∫æT S·ª¨A V53e) ---
            
        return unscaled_close
        
    except Exception as e:
        logging.error(f"L·ªói 'Unscale': {e}")
        if scaled_data_np.ndim == 3:
            return np.zeros(scaled_data_np.shape[1]) 
        else:
            return np.zeros(LOOKFORWARD)

# --- (S·ª¨A V53j: THU·∫¨T TO√ÅN T√åM KI·∫æM TH√îNG MINH - TR√ÅNH TR√ôNG L·∫∂P) ---
def find_top_3_similar_patterns(current_window_raw_close, all_historical_close_series, lookback, lookforward):
    """
    D√πng "chi√™u" stumpy.mass ƒë·ªÉ "qu√©t" Top 3 "Anh Em Song Sinh" (ƒê√É N√ÇNG C·∫§P V53j)
    """
    logging.info(f"(LB={lookback}) ƒêang 'qu√©t' {len(all_historical_close_series)} n·∫øn qu√° kh·ª© ƒë·ªÉ t√¨m 'Top 3 Anh Em'...")
    
    # 1. "M·∫´u" (Query) (ƒë√£ Z-Score)
    query_window = (current_window_raw_close - np.mean(current_window_raw_close)) / (np.std(current_window_raw_close) + 1e-9)
    
    # 2. "L·ªãch S·ª≠" (Search)
    history_to_search_series = all_historical_close_series.iloc[:-(lookback + lookforward)]
    history_to_search_values = history_to_search_series.values
    
    # 3. "Qu√©t"
    try:
        distance_profile = stumpy.mass(query_window, history_to_search_values)
    except Exception as e:
        logging.error(f"(LB={lookback}) L·ªói khi 'qu√©t' stumpy.mass: {e}.")
        return "L·ªói: Kh√¥ng th·ªÉ 'qu√©t' qu√° kh·ª©.", []
        
    # --- (B·∫ÆT ƒê·∫¶U N√ÇNG C·∫§P V53j: C∆† CH·∫æ "V√ôNG C·∫§M ƒê·ªäA") ---
    top_3_matches = []
    dist_profile_copy = distance_profile.copy() # Copy ƒë·ªÉ "ph√°" m√† kh√¥ng ·∫£nh h∆∞·ªüng g·ªëc
    
    # B√°n k√≠nh v√πng c·∫•m (ƒë·ªÉ 2 pattern kh√¥ng b·ªã "d√≠nh" nhau qu√° g·∫ßn)
    exclusion_zone = lookback // 2 
    
    for _ in range(3): # T√¨m 3 th·∫±ng
        # T√¨m th·∫±ng nh·ªè nh·∫•t (gi·ªëng nh·∫•t) hi·ªán t·∫°i
        idx = np.argmin(dist_profile_copy)
        score = dist_profile_copy[idx]
        
        # N·∫øu "h·∫øt h√†ng" (to√†n v√¥ c·ª±c) th√¨ ngh·ªâ
        if score == np.inf: break

        timestamp = history_to_search_series.index[idx]
        top_3_matches.append({'index': idx, 'timestamp': timestamp, 'score': score})

        # "Khoanh v√πng c·∫•m ƒë·ªãa" (ƒê·∫∑t distance xung quanh idx th√†nh V√¥ C·ª±c)
        # ƒê·ªÉ l·∫ßn l·∫∑p sau kh√¥ng t√¨m th·∫•y n√≥ n·ªØa
        start_ex = max(0, idx - exclusion_zone)
        end_ex = min(len(dist_profile_copy), idx + exclusion_zone)
        dist_profile_copy[start_ex:end_ex] = np.inf
        
    # --- (K·∫æT TH√öC N√ÇNG C·∫§P V53j) ---
    
    similarity_text = f"Top 3 T∆∞∆°ng ƒê·ªìng (LB={lookback}):"
    if top_3_matches:
        similarity_text += f"\n  1. {top_3_matches[0]['timestamp'].strftime('%Y-%m-%d %H:00')} (Score: {top_3_matches[0]['score']:.2f})"
        logging.info(f"--- (LB={lookback}) {similarity_text} ---")
            
    return similarity_text, top_3_matches

# --- (S·ª¨A V53L: GIAO DI·ªÜN OVERLAY VOLUME) ---
def draw_super_chart(data_lb50, data_lb168, df_master_full):
    """
    V·∫º BI·ªÇU ƒê·ªí "SI√äU C·∫§P" (2 PH·∫¶N: 50 vs 168) - TradingView Style (Volume Overlay)
    """
    logging.info(f"ƒêang 'v·∫Ω' bi·ªÉu ƒë·ªì 'Si√™u C·∫•p' (TradingView Style) -> {CHART_FILENAME}...")
    
    # V53L: Ch·ªâ d√πng 2 h√†ng, kh√¥ng t√°ch volume ri√™ng n·ªØa
    fig = plt.figure(figsize=(40, 16)) 
    gs = GridSpec(2, 4, figure=fig, width_ratios=[3, 1, 1, 1])
    
    # --- T·∫°o Axes (Ch·ªâ c·∫ßn 1 ax cho m·ªói chart) ---
    axes_50 = [fig.add_subplot(gs[0, i]) for i in range(4)]
    axes_168 = [fig.add_subplot(gs[1, i]) for i in range(4)]
    
    plot_chart_section(axes_50, data_lb50, 50, df_master_full)
    plot_chart_section(axes_168, data_lb168, 168, df_master_full)
    
    fig.suptitle(f"BOT LIVE V53p (Final No Gap) - {pd.Timestamp.now(tz='UTC').strftime('%Y-%m-%d %H:%M:%S UTC')}", fontsize=20, weight='bold')
    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) 
    plt.savefig(CHART_FILENAME)
    plt.close(fig) 
    logging.info(f"ƒê√£ 'v·∫Ω' (TradingView Style) v√† l∆∞u xong bi·ªÉu ƒë·ªì t·∫°i: {CHART_FILENAME}")

# --- (S·ª¨A V53m/p: V√Å L·ªñI "KHE H·ªû" + "C·∫ÆT ƒêU√îI TH·ª™A") ---
def plot_chart_section(axes, data, lookback, df_master_full):
    """
    H√†m "V·∫Ω Con" V53L: V·∫Ω Gi√° & Volume (Overlay) tr√™n c√πng 1 √¥
    """
    past_ohlcv = data['past_ohlcv']
    scenarios = data['scenarios']
    similarity_text = data['similarity_text']
    top_3_matches = data['top_3_matches']
    
    # === 1. V·∫º MAIN CHART (Hi·ªán t·∫°i) ===
    ax_main = axes[0] 
    
    # (FIX V53p: D·ªçn s·∫°ch d·ªØ li·ªáu r√°c NaN tr∆∞·ªõc khi v·∫Ω ƒë·ªÉ len() chu·∫©n x√°c)
    past_ohlcv = past_ohlcv.dropna(subset=['H1_Close', 'H1_Open', 'H1_High', 'H1_Low'])
    
    # ƒê·ªïi t√™n c·ªôt cho mplfinance
    past_data_mpf = past_ohlcv.rename(columns={
        'H1_Open': 'Open', 'H1_High': 'High', 'H1_Low': 'Low', 'H1_Close': 'Close', 'H1_Volume': 'Volume'
    })
    
    # (A) V·∫Ω N·∫øn (Gi√°)
    mpf.plot(past_data_mpf, type='candle', ax=ax_main, style='yahoo', volume=False, show_nontrading=False)
    
    # (B) V·∫Ω Volume Overlay
    ax_vol = ax_main.twinx()
    vol_colors = np.where(past_data_mpf['Close'] >= past_data_mpf['Open'], 'green', 'red')
    ax_vol.bar(np.arange(len(past_data_mpf)), past_data_mpf['Volume'], color=vol_colors, alpha=0.3, width=0.6)
    max_vol = past_data_mpf['Volume'].max()
    if max_vol > 0:
        ax_vol.set_ylim(0, max_vol * 4)
    ax_vol.axis('off') 
    ax_main.set_zorder(2) 
    ax_main.patch.set_visible(False)
    ax_vol.set_zorder(1)

    # (C) V·∫Ω 3 K·ªãch b·∫£n D·ª± b√°o (V53i Logic + V53m Fix Gap)
    last_close_price = past_data_mpf['Close'].iloc[-1]
    
    # --- (S·ª¨A V53m: T√çNH TO√ÅN ƒêI·ªÇM B·∫ÆT ƒê·∫¶U V·∫º D·ª∞A TR√äN DATA TH·ª∞C T·∫æ) ---
    len_data = len(past_data_mpf)
    plot_x = np.arange(len_data - 1, len_data - 1 + (LOOKFORWARD + 1)) 
    # ---------------------------------------------------
    
    colors = {'CVAE-LSTM': 'blue', 'TCVAE': 'red', 'TimeGAN': 'green'}
    
    for name, sim_scaled in scenarios.items():
        sim_unscaled = unscale_h1_close(sim_scaled)
        if len(sim_unscaled) != LOOKFORWARD: continue 
             
        try:
            if name == 'TimeGAN':
                sim_series = pd.Series(sim_unscaled)
                sim_unscaled = sim_series.ewm(span=3, adjust=False).mean().values 
                
            start_price_sim = sim_unscaled[0]
            if start_price_sim == 0: start_price_sim = 1e-9
            growth_factors = sim_unscaled / start_price_sim
            
            if name == 'TimeGAN':
                growth_factors = 1.0 + (growth_factors - 1.0) * TIMEGAN_DAMPING_FACTOR 
            
            sim_unscaled = last_close_price * growth_factors
            
        except Exception: pass
             
        plot_line = np.insert(sim_unscaled, 0, last_close_price)
        
        if len(plot_x) == len(plot_line):
            ax_main.plot(plot_x, plot_line, color=colors[name], linestyle='--', linewidth=2, label=f'KB {name}')
    
    ax_main.set_title(f"PH·∫¶N {lookback} (Hi·ªán T·∫°i & D·ª± B√°o)\n{similarity_text}", fontsize=14, loc='left')
    ax_main.legend(loc='upper left'); ax_main.grid(True)
    
    # === 2. V·∫º 3 CHART PH·ª§ (TOP MATCHES) ===
    for i, match in enumerate(top_3_matches):
        ax_sub = axes[i+1] 
        
        start_idx = match['index']
        end_idx = match['index'] + lookback + LOOKFORWARD 
        segment_data_raw = df_master_full.iloc[start_idx:end_idx]
        segment_data = segment_data_raw[['H1_Open', 'H1_High', 'H1_Low', 'H1_Close', 'H1_Volume']]
        segment_data.columns = ['Open', 'High', 'Low', 'Close', 'Volume']
        
        v_line_time = segment_data.index[lookback-1]
        mpf.plot(segment_data, type='candle', ax=ax_sub, style='yahoo', volume=False, 
                 vlines=dict(vlines=[v_line_time], linestyle='--', colors='b', linewidths=2),
                 show_nontrading=False)
        
        ax_sub_vol = ax_sub.twinx()
        vol_colors_sub = np.where(segment_data['Close'] >= segment_data['Open'], 'green', 'red')
        ax_sub_vol.bar(np.arange(len(segment_data)), segment_data['Volume'], color=vol_colors_sub, alpha=0.3, width=0.6)
        
        max_vol_sub = segment_data['Volume'].max()
        if max_vol_sub > 0:
            ax_sub_vol.set_ylim(0, max_vol_sub * 4) 
        ax_sub_vol.axis('off')
        ax_sub.set_zorder(2)
        ax_sub.patch.set_visible(False)
        ax_sub_vol.set_zorder(1)

        ax_sub.set_title(f"Top {i+1} (Score: {match['score']:.2f})\n{match['timestamp'].strftime('%Y-%m-%d')}", fontsize=12)
        ax_sub.yaxis.tick_right()
        
# ---------------------------------------------------


# --- (S·ª¨A V53_Update_3H) H√ÄM CH√çNH (Ch·∫°y 6 N√£o + Soi l·∫°i 3h) ---
def run_hourly_update_and_predict(data_service):
    """
    H√†m CH√çNH: Ch·∫°y to√†n b·ªô 6 "N√£o" m·ªói gi·ªù.
    (ƒê√£ ƒë·ªô th√™m t√≠nh nƒÉng refresh data 3h tr∆∞·ªõc khi v·∫Ω)
    """
    logging.info("=== B·∫ÆT ƒê·∫¶U CHU K·ª≤ 1H M·ªöI (V53 - 6 N√ÉO) ===")
    
    # --- B∆∞·ªõc 0: T√≠nh to√°n m·ªëc "H·ªìi Quy" (3 Gi·ªù Tr∆∞·ªõc) ---
    # M·ª•c ƒë√≠ch: √âp bot t·∫£i l·∫°i n·∫øn c·ªßa 3 ti·∫øng g·∫ßn nh·∫•t ƒë·ªÉ tr√°m l·ªó h·ªïng (n·∫øu c√≥)
    timestamp_3h_str = None
    try:
        now_utc = pd.Timestamp.now(tz='UTC')
        time_3h_ago = now_utc - pd.Timedelta(hours=3)
        # Chuy·ªÉn th√†nh timestamp ms (d√†nh cho ccxt/binance)
        timestamp_3h_ms = int(time_3h_ago.timestamp() * 1000)
        timestamp_3h_str = str(timestamp_3h_ms)
        logging.info(f"Bot ƒëang y√™u c·∫ßu l√†m m·ªõi d·ªØ li·ªáu t·ª´: {time_3h_ago.strftime('%H:%M')} UTC (3h tr∆∞·ªõc)...")
    except Exception as e:
        logging.error(f"L·ªói t√≠nh gi·ªù h·ªìi quy: {e}. S·∫Ω ch·∫°y m·∫∑c ƒë·ªãnh.")

    # --- B∆∞·ªõc 1 & 2: C·∫≠p nh·∫≠t v√† G·ªôp "Th·ª©c ƒÉn" ---
    logging.info("B∆∞·ªõc 1&2: ƒêang c·∫≠p nh·∫≠t & t√°i t·∫°o 'th·ª©c ƒÉn' master (53 m√≥n)...")
    try:
        # (NEW) C·ªë g·∫Øng g·ªçi download v·ªõi tham s·ªë start_str
        if timestamp_3h_str:
            try:
                # Hy v·ªçng data_service c·ªßa ƒë·∫°i ca kh√¥n, bi·∫øt ƒÉn tham s·ªë 'start_str'
                data_service.run_download_klines(start_str=timestamp_3h_str)
                logging.info("‚úÖ ƒê√£ g·ªçi download l·∫°i 3 gi·ªù g·∫ßn nh·∫•t th√†nh c√¥ng!")
            except TypeError:
                # N·∫øu data_service "ngu" kh√¥ng nh·∫≠n tham s·ªë -> Ch·∫°y ki·ªÉu c≈©
                logging.warning("‚ö†Ô∏è DataService kh√¥ng nh·∫≠n tham s·ªë 'start_str'. Ch·∫°y ch·∫ø ƒë·ªô m·∫∑c ƒë·ªãnh...")
                data_service.run_download_klines()
            except Exception as e:
                logging.error(f"‚ö†Ô∏è L·ªói khi force download 3h: {e}. Th·ª≠ ch·∫°y m·∫∑c ƒë·ªãnh...")
                data_service.run_download_klines()
        else:
            # Kh√¥ng t√≠nh ƒë∆∞·ª£c gi·ªù th√¨ ch·∫°y nh∆∞ c≈©
            data_service.run_download_klines()

        data_service.run_fetch_bitstamp_backfill() # (Ch·∫°y L√≤ 1.5)
        data_service.run_create_master_file() # (Ch·∫°y L√≤ 2)
    except Exception as e:
        logging.error(f"L·ªñI \"H√öT\" DATA: {e}. Bot s·∫Ω \"ƒÉn\" data \"c≈©\" (n·∫øu c√≥).")

    # --- B∆∞·ªõc 3: ƒê·ªçc "Th·ª©c ƒÉn" 1 L·∫¶N DUY NH·∫§T ---
    logging.info("B∆∞·ªõc 3: ƒê√£ c√≥ 'th·ª©c ƒÉn' m·ªõi nh·∫•t. B·∫Øt ƒë·∫ßu \"ƒÉn\" (load)...")
    
    try:
        df_master_full = pd.read_parquet(os.path.join('02_Master_Data', 'btcusdt_master_data.parquet'))
    except Exception as e:
        logging.error(f"L·ªói khi ƒë·ªçc file 'th·ª©c ƒÉn' master 'btcusdt_master_data.parquet': {e}")
        return

    # --- (S·ª¨A L·ªñI V53c: "V·ªÜ SINH" "TH·ª®C ƒÇN") ---
    # (B√™ nguy√™n "b√†i" 'v·ªá sinh' t·ª´ file "plot" sang)
    logging.info("B∆∞·ªõc 3.5: ƒêang \"v·ªá sinh\" (interpolate/fillna) to√†n b·ªô 'th·ª©c ƒÉn'...")
    df_master_full.interpolate(method='time', inplace=True)
    df_master_full.fillna(method='ffill', inplace=True)
    df_master_full.fillna(method='bfill', inplace=True)
    df_master_full.fillna(0, inplace=True) # "Tr√°m" 0 (n·∫øu v·∫´n c√≤n)
    logging.info("ƒê√£ \"v·ªá sinh\" 'th·ª©c ƒÉn' xong. 'M·ªìi' 'b√¢y gi·ªù' 'si√™u' 's·∫°ch'!")
    # --- (H·∫øt L·ªói V53c) ---

    # === (S·ª¨A V53) B∆∞·ªõc 4: "Ch·∫°y" "Ph·∫ßn" 1 (Lookback 50) ===
    logging.info("--- B·∫Øt ƒë·∫ßu \"Ph·∫ßn 1\" (Lookback 50) ---")
    data_lb50 = {}
    
    # 4.1. "M√≥c m·ªìi" 50 n·∫øn (B√¢y gi·ªù "m·ªìi" ƒë√£ "s·∫°ch")
    window_scaled_50, past_ohlcv_50 = get_current_window_scaled_from_df(df_master_full, 50)
    
    if window_scaled_50 is None:
        logging.warning("Kh√¥ng th·ªÉ chu·∫©n b·ªã d·ªØ li·ªáu LB=50. B·ªè qua chu k·ª≥ n√†y.")
        return
        
    data_lb50['past_ohlcv'] = past_ohlcv_50
    data_lb50['scenarios'] = {}

    # 4.2. "V·∫Ω" (Generate) 3 "n√£o" 50
    data_lb50['scenarios']['CVAE-LSTM'] = get_mean_scenario(CVAE_LSTM_50, window_scaled_50)
    data_lb50['scenarios']['TimeGAN'] = get_timegan_scenario(TIMEGAN_G_50, TIMEGAN_R_50, 50)
    data_lb50['scenarios']['TCVAE'] = get_mean_scenario(TCVAE_50, window_scaled_50)
    logging.info("(LB=50) ƒê√£ \"v·∫Ω\" xong 3 k·ªãch b·∫£n.")

    # 4.3. "Soi" (Scan) Top 3 "Anh Em" 50
    all_historical_close = df_master_full['H1_Close']
    current_window_close_50 = past_ohlcv_50['H1_Close'].values
    sim_text_50, top_3_50 = find_top_3_similar_patterns(current_window_close_50, all_historical_close, 50, LOOKFORWARD)
    data_lb50['similarity_text'] = sim_text_50
    data_lb50['top_3_matches'] = top_3_50
    
    # === (S·ª¨A V53) B∆∞·ªõc 5: "Ch·∫°y" "Ph·∫ßn" 2 (Lookback 168) ===
    logging.info("--- B·∫Øt ƒë·∫ßu \"Ph·∫ßn 2\" (Lookback 168) ---")
    data_lb168 = {}
    
    # 5.1. "M√≥c m·ªìi" 168 n·∫øn (B√¢y gi·ªù "m·ªìi" ƒë√£ "s·∫°ch")
    window_scaled_168, past_ohlcv_168 = get_current_window_scaled_from_df(df_master_full, 168)
    
    if window_scaled_168 is None:
        logging.warning("Kh√¥ng th·ªÉ chu·∫©n b·ªã d·ªØ li·ªáu LB=168. B·ªè qua chu k·ª≥ n√†y.")
        return
        
    data_lb168['past_ohlcv'] = past_ohlcv_168
    data_lb168['scenarios'] = {}

    # 5.2. "V·∫Ω" (Generate) 3 "n√£o" 168
    data_lb168['scenarios']['CVAE-LSTM'] = get_mean_scenario(CVAE_LSTM_168, window_scaled_168)
    data_lb168['scenarios']['TimeGAN'] = get_timegan_scenario(TIMEGAN_G_168, TIMEGAN_R_168, 168)
    data_lb168['scenarios']['TCVAE'] = get_mean_scenario(TCVAE_168, window_scaled_168)
    logging.info("(LB=168) ƒê√£ \"v·∫Ω\" xong 3 k·ªãch b·∫£n.")

    # 5.3. "Soi" (Scan) Top 3 "Anh Em" 168
    current_window_close_168 = past_ohlcv_168['H1_Close'].values
    sim_text_168, top_3_168 = find_top_3_similar_patterns(current_window_close_168, all_historical_close, 168, LOOKFORWARD)
    data_lb168['similarity_text'] = sim_text_168
    data_lb168['top_3_matches'] = top_3_168
    
    # === B∆∞·ªõc 6: "V·∫Ω" (Plot) "Si√™u ·∫¢nh" ===
    draw_super_chart(data_lb50, data_lb168, df_master_full)
    
    logging.info("=== Ho√†n t·∫•t chu k·ª≥ ===")

# --- H√ÄM MAIN ƒê·ªÇ CH·∫†Y FILE ---
if __name__ == "__main__":
    
    # (B·ªè Argparser - Bot V53 "t·ª± ƒë·ªông" "ƒÉn" 50 v√† 168)
    
    # 1. Kh·ªüi t·∫°o "H·∫≠u c·∫ßn" (Data Service V23)
    data_service = MasterDataServiceV23(symbol='BTCUSDT')
    
    # 2. Load "N√£o B·ªô" 1 L·∫¶N DUY NH·∫§T (6 "N√£o")
    if not load_all_brains():
        logging.critical("Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông bot. D·ª´ng l·∫°i.")
        exit()
        
    # 3. L·∫≠p L·ªãch Ch·∫°y
    logging.info("Bot kh·ªüi ƒë·ªông. L·∫≠p l·ªãch ch·∫°y...")
    
    # 3.1. Ch·∫°y 1 l·∫ßn ngay l√∫c ƒë·∫ßu
    run_hourly_update_and_predict(data_service)
    
    # 3.2. L·∫≠p l·ªãch ch·∫°y v√†o ph√∫t th·ª© 2 c·ªßa m·ªói gi·ªù
    logging.info("ƒê√£ l·∫≠p l·ªãch ch·∫°y v√†o ph√∫t :02 m·ªói gi·ªù...")
    schedule.every().hour.at(":02").do(
        run_hourly_update_and_predict, 
        data_service=data_service
    )
    
    # 4. V√≤ng l·∫∑p "S·ªëng"
    while True:
        schedule.run_pending()
        time.sleep(1)