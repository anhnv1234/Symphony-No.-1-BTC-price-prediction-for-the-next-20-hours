import pandas as pd
import numpy as np
import os
import logging
import joblib
from sklearn.preprocessing import MinMaxScaler
import warnings 
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm
import sys 
import matplotlib.pyplot as plt 
import glob 
import math # (C·∫ßn cho "N√£o" Transformer)

# --- (M√ìN 1) "B·ªäT MI·ªÜNG" TO√ÄN B·ªò C·∫¢NH B√ÅO "R√ÅC" ---
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
# --- (H·∫æT M√ìN 1) ---

# (Ki·ªÉm tra TensorBoard)
try:
    from torch.utils.tensorboard import SummaryWriter
except ImportError:
    SummaryWriter = None 

psutil = None 
pynvml = None
GPU_HANDLE = None

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- H·∫±ng s·ªë "C√îNG X∆Ø·ªûNG" V13 (TCVAE - Transformer CVAE) ---
MASTER_FILE_PATH = os.path.join('02_Master_Data', 'btcusdt_master_data.parquet') 
SCALER_FILENAME = os.path.join('01_Processed_Data', 'cvae_scaler_V23.gz')
LOOKFORWARD = 20 
LATENT_DIM = 32
BETA_VALUE = 0.01 
BETA_ANNEAL_EPOCHS = 50 
GRAD_CLIP_VALUE = 1.0    
WEIGHTED_LOSS_MULTIPLIER = 10.0 
DEFAULT_SOI_EPOCHS = 150 
DEFAULT_TENSORBOARD_DIR = "runs_v13_transformer_cvae" 
DIR_MODELS = "03_Models"
DIR_PROCESSED = "01_Processed_Data"
# --- (S·ª¨A V13: C·∫§U H√åNH "N√ÉO" TRANSFORMER) ---
NUM_FEATURES_GOC = 53 
D_MODEL = 64      
N_HEAD = 4        
NUM_ENC_LAYERS = 2 
NUM_DEC_LAYERS = 2 
# (Bi·∫øn "to√†n c·ª•c" ƒë·ªÉ "v·∫Ω")
CVAE_CLOSE_IDX_FOR_PLOT = -1
SCALER_MIN_CLOSE = 0.0
SCALER_SCALE_CLOSE = 1.0

# --- (H√†m ph·ª• "worker_init_fn", "HardwareMonitor": Gi·ªØ nguy√™n) ---
def worker_init_fn(worker_id):
    warnings.filterwarnings("ignore", category=FutureWarning)
    logging.getLogger().setLevel(logging.ERROR) 
class HardwareMonitor:
    def __init__(self, device_type):
        self.device_type = device_type
        self.is_gpu = (device_type == "cuda" and pynvml is not None)
    def get_stats(self):
        if self.is_gpu:
            try:
                util = pynvml.nvmlDeviceGetUtilizationRates(GPU_HANDLE)
                gpu_load = f"{util.gpu}%"; mem = pynvml.nvmlDeviceGetMemoryInfo(GPU_HANDLE)
                vram_used_gb = mem.used / (1024**3); vram_total_gb = mem.total / (1024**3)
                vram_load = f"{vram_used_gb:.1f}/{vram_total_gb:.1f}GB"
                return {"GPU": gpu_load, "VRAM": vram_load}
            except Exception: self.is_gpu = False ; return self.get_stats() 
        else:
            if psutil: return {"CPU": f"{psutil.cpu_percent()}%"}
            else: return {"CPU": "N/A"}

# --- (H√†m "load_data_and_scaler_V23": Gi·ªØ nguy√™n) ---
def load_data_and_scaler_V23():
    logging.info(f"ƒêang t·∫£i file master V23 (53 m√≥n): {MASTER_FILE_PATH}...")
    try:
        df = pd.read_parquet(MASTER_FILE_PATH)
    except FileNotFoundError:
        logging.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y file {MASTER_FILE_PATH}.")
        return None, None, None
        
    logging.info(f"ƒêang t·∫£i 'B·ªô Chu·∫©n H√≥a' (V23 - 53 m√≥n) (d√πng chung CVAE): {SCALER_FILENAME}...")
    try:
        scaler = joblib.load(SCALER_FILENAME)
    except FileNotFoundError:
        logging.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y file {SCALER_FILENAME}.")
        logging.error("ƒê·∫°i ca ƒë√£ ch·∫°y 'L√≤' (train_cvae_V11.py) (file 'ƒÉn' 53 m√≥n) [B∆∞·ªõc 1] ch∆∞a?")
        return None, None, None
    
    df.interpolate(method='time', inplace=True)
    df.fillna(method='ffill', inplace=True)
    df.fillna(method='bfill', inplace=True)
    df.fillna(0, inplace=True) 

    features_df = df
    feature_names = features_df.columns.tolist()
    num_features = len(feature_names)
    
    if num_features != NUM_FEATURES_GOC:
        logging.error(f"L·ªñI 'KH·ªöP' N√ÉO: 'L√≤' V13 'code' 53 m√≥n, nh∆∞ng file 'g·ªëc' 'c√≥' {num_features} m√≥n.")
        return None, None, None

    logging.info("ƒêang chu·∫©n h√≥a 'th·ª©c ƒÉn' (d√πng Scaler L√≤ V11)...")
    data_scaled = scaler.transform(features_df)
    
    return data_scaled, num_features, feature_names

# --- (H√†m "create_windows": Gi·ªØ nguy√™n) ---
def create_windows(data_scaled, lookback, lookforward):
    logging.info(f"ƒêang 'c·∫Øt' c·ª≠a s·ªï CVAE (Lookback={lookback})...")
    X_past, Y_future = [], []
    num_samples = len(data_scaled) - lookback - lookforward + 1
    if num_samples <= 0:
        logging.error(f"L·ªñI (Lookback={lookback}): D·ªØ li·ªáu qu√° √≠t.")
        return None, None
    for i in tqdm(range(num_samples), desc=f"C·∫Øt c·ª≠a s·ªï CVAE LB={lookback}"):
        X_past.append(data_scaled[i : i + lookback])
        Y_future.append(data_scaled[i + lookback : i + lookback + lookforward])
    X_past = np.array(X_past); Y_future = np.array(Y_future)
    logging.info(f"Shape X_past: {X_past.shape} | Shape Y_future: {Y_future.shape}")
    return X_past, Y_future

# --- (H√†m "sampling", "augment_batch": Gi·ªØ nguy√™n) ---
def sampling(args):
    z_mean, z_log_var = args
    batch = z_mean.shape[0]; dim = z_mean.shape[1]
    epsilon = torch.randn(size=(batch, dim)).to(device)
    return z_mean + torch.exp(0.5 * z_log_var) * epsilon

def augment_batch(batch_data, jitter_strength=0.01, scale_strength=0.1):
    B, L, D = batch_data.shape
    jitter = torch.randn_like(batch_data) * jitter_strength
    scaling_factor = 1.0 + (torch.rand((B, 1, 1), device=batch_data.device) - 0.5) * 2 * scale_strength
    augmented_batch = (batch_data + jitter) * scaling_factor
    return torch.clamp(augmented_batch, 0.0, 1.0)


# =========================================================================
# üí° B∆Ø·ªöC 1: "ƒê·ªò" N√ÉO TRANSFORMER (TCVAE V1)
# =========================================================================

class PositionalEncoding(nn.Module):
    """ "Nh·ªìi" v·ªã tr√≠ (th·ª© t·ª± 1, 2, 3...) v√†o "n√£o" """
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        pe = pe.permute(1, 0, 2) # (Shape: 1, max_len, d_model) (S·ª≠a cho batch_first)
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ x: Shape (Batch, Seq_Len, d_model) """
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)

# --- (S·ª¨A V13: "ƒê·ªò" N√ÉO ENCODER D√ôNG TRANSFORMER) ---
class Encoder(nn.Module):
    def __init__(self, lookback, lookforward, num_features, d_model, n_head, num_enc_layers, latent_dim):
        super(Encoder, self).__init__()
        
        self.embed_past = nn.Linear(num_features, d_model)
        self.embed_future = nn.Linear(num_features, d_model)
        self.pos_encoder_past = PositionalEncoding(d_model, max_len=lookback)
        self.pos_encoder_future = PositionalEncoding(d_model, max_len=lookforward)
        encoder_layer_past = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head, batch_first=True)
        self.transformer_encoder_past = nn.TransformerEncoder(encoder_layer_past, num_layers=num_enc_layers)
        encoder_layer_future = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head, batch_first=True)
        self.transformer_encoder_future = nn.TransformerEncoder(encoder_layer_future, num_layers=num_enc_layers)
        self.flatten = nn.Flatten()
        self.dense_combine = nn.Linear(d_model * (lookback + lookforward), 128)
        self.z_mean = nn.Linear(128, latent_dim)
        self.z_log_var = nn.Linear(128, latent_dim)
        self.relu = nn.ReLU()

    def forward(self, condition_input, future_input): 
        x_past = self.embed_past(condition_input) 
        x_future = self.embed_future(future_input) 
        x_past = self.pos_encoder_past(x_past)
        x_future = self.pos_encoder_future(x_future)
        past_features = self.transformer_encoder_past(x_past) 
        future_features = self.transformer_encoder_future(x_future) 
        combined = torch.cat([past_features, future_features], dim=1) 
        combined_flat = self.flatten(combined) 
        h_combined = self.relu(self.dense_combine(combined_flat)) 
        mean = self.z_mean(h_combined)
        log_var = self.z_log_var(h_combined)
        return mean, log_var

# --- (S·ª¨A V13: "ƒê·ªò" N√ÉO DECODER D√ôNG TRANSFORMER) ---
class Decoder(nn.Module):
    # === ("V√Å" 1/2) ===
    def __init__(self, lookback, lookforward, num_features, d_model, n_head, num_enc_layers, num_dec_layers, latent_dim):
        super(Decoder, self).__init__()
        self.lookforward = lookforward
        self.d_model = d_model
        self.embed_past = nn.Linear(num_features, d_model)
        self.z_embed = nn.Linear(latent_dim, d_model)
        self.past_feature_embed = nn.Linear(d_model * lookback, d_model)
        self.pos_encoder_past = PositionalEncoding(d_model, max_len=lookback)
        self.pos_encoder_future_query = PositionalEncoding(d_model, max_len=lookforward) 
        encoder_layer_past = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head, batch_first=True)
        self.transformer_encoder_past = nn.TransformerEncoder(encoder_layer_past, num_layers=num_enc_layers)
        self.dense_upsample = nn.Linear(d_model + d_model, d_model * lookforward)
        decoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head, batch_first=True)
        self.transformer_decoder = nn.TransformerEncoder(decoder_layer, num_layers=num_dec_layers)
        self.output_layer = nn.Linear(d_model, num_features)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, condition_input, latent_input):
        x_past = self.embed_past(condition_input) 
        x_past = self.pos_encoder_past(x_past)
        past_features_seq = self.transformer_encoder_past(x_past) 
        past_features_flat = past_features_seq.mean(dim=1) 
        z_features = self.relu(self.z_embed(latent_input)) 
        combined = torch.cat([past_features_flat, z_features], dim=1) 
        upsampled_features = self.relu(self.dense_upsample(combined))
        future_query = upsampled_features.view(-1, self.lookforward, self.d_model)
        future_query = self.pos_encoder_future_query(future_query)
        reconstruction_features = self.transformer_decoder(future_query) 
        reconstruction = self.sigmoid(self.output_layer(reconstruction_features)) 
        return reconstruction, None 
# =========================================================================

# --- (H√ÄM "V·∫º" (PLOT) V10.2) ---
def plot_cvae_samples(X_past_sample, Y_future_sample, encoder, decoder, writer, epoch, lookback, num_scenarios=5):
    try:
        encoder.eval(); decoder.eval()
        X_past_gpu = X_past_sample.unsqueeze(0).to(device) 
        Y_future_gpu = Y_future_sample.unsqueeze(0).to(device) 
        if CVAE_CLOSE_IDX_FOR_PLOT == -1:
             logging.warning("Kh√¥ng 'v·∫Ω' PNG ƒë∆∞·ª£c v√¨ CVAE_CLOSE_IDX_FOR_PLOT ch∆∞a ƒë∆∞·ª£c set.")
             return
        y_true_scaled = Y_future_gpu[0, :, CVAE_CLOSE_IDX_FOR_PLOT].cpu().numpy()
        y_true_unscaled = (y_true_scaled * SCALER_SCALE_CLOSE) + SCALER_MIN_CLOSE
        fig, ax = plt.subplots(figsize=(15, 7))
        for _ in range(num_scenarios):
            with torch.no_grad():
                z_mean, z_log_var = encoder(X_past_gpu, Y_future_gpu)
                z = sampling((z_mean, z_log_var))
                reconstruction, _ = decoder(X_past_gpu, z)
            y_pred_scaled = reconstruction[0, :, CVAE_CLOSE_IDX_FOR_PLOT].cpu().numpy()
            y_pred_unscaled = (y_pred_scaled * SCALER_SCALE_CLOSE) + SCALER_MIN_CLOSE
            ax.plot(y_pred_unscaled, color='blue', alpha=0.3)
        ax.plot(y_true_unscaled, color='red', linewidth=2, label='H√†ng Th·∫≠t (Y_future)')
        ax.set_title(f"Ph√≤ng Tri·ªÉn L√£m (V13 TCVAE 'Si√™u C·∫•p' - LB={lookback}): 5 K·ªãch B·∫£n H1_Close (Epoch {epoch} - X·ªãn Nh·∫•t)")
        ax.set_xlabel(f"{LOOKFORWARD} n·∫øn H1 t∆∞∆°ng lai"); ax.set_ylabel("Gi√° H1_Close (USDT)"); ax.legend()
        if writer: writer.add_figure(f'Sample_Plots/K·ªãch_B·∫£n_V·∫Ω_LB{lookback}', fig, global_step=epoch)
        chart_save_path = os.path.join(DIR_MODELS, f"cvae_samples_V13_TCVAE_lb{lookback}_epoch{epoch}_BEST.png")
        try: fig.savefig(chart_save_path, dpi=150, bbox_inches='tight') 
        except Exception as e: logging.warning(f"L·ªói l∆∞u chart Samples PNG: {e}")
        plt.close(fig) 
    except Exception as e: logging.warning(f"L·ªói 'V·∫Ω' K·ªãch B·∫£n (plot_cvae_samples): {e}")

def plot_attention_heatmap(attn_weights_plot_numpy, writer, epoch, lookback):
    pass # (V13 kh√¥ng "v·∫Ω" Attn)

# --- (H√ÄM "L√ïI" V13 - "God Function") ---
def train_single_cvae_pytorch(X_past, Y_future, lookback, lookforward, num_features, weights_tensor, mode_config):
    
    val_split = mode_config['val_split']
    total_epochs = mode_config['epochs'] 
    log_dir = mode_config['log_dir']
    final_model_name = mode_config['final_name'].format(lb=lookback, lf=lookforward) 
    
    # === ("V√Å" L·ªñI 1/2) ===
    resume_file_name = None
    if mode_config.get('resume_name'):
        resume_file_name = mode_config.get('resume_name').format(lb=lookback)
    # =====================
    
    use_patience = mode_config['use_patience']
    
    writer = None
    if log_dir and SummaryWriter: 
        writer = SummaryWriter(log_dir=os.path.join(log_dir, f'cvae_v13_lb_{lookback}'))
    
    pin_mem = (device.type == 'cuda')
    X_past_t = torch.tensor(X_past, dtype=torch.float32)
    Y_future_t = torch.tensor(Y_future, dtype=torch.float32)
    
    val_size = int(len(X_past_t) * val_split) 
    train_size = len(X_past_t) - val_size
    
    X_past_train, X_past_val = X_past_t[:train_size], X_past_t[train_size:]
    Y_future_train, Y_future_val = Y_future_t[:train_size], Y_future_t[train_size:]
    
    logging.info(f"T√°ch Train/Val (V13): {train_size} (train) / {val_size} (val)")

    # === ("V√Å" L·ªñI 3/3 - "B√ìP M·∫∫" (OOM)) ===
    # (N·∫øu "n√£o" "to" (LB=168), "ƒÉn" "m·∫ª" "nh·ªè" (32) ƒë·ªÉ "ch·ªëng" OOM)
    if lookback > 100:
        current_batch_size = 32
        logging.warning(f"Lookback 'To' ({lookback}). 'B√≥p' Batch Size xu·ªëng {current_batch_size} ƒë·ªÉ 'ch·ªëng' OOM.")
    else:
        current_batch_size = 64
        logging.info(f"Lookback 'Nh·ªè' ({lookback}). D√πng Batch Size {current_batch_size}.")
    # =======================================

    train_dataset = TensorDataset(X_past_train, Y_future_train)
    train_loader = DataLoader(train_dataset, batch_size=current_batch_size, shuffle=True, # (S·ª¨A V13: D√πng "m·∫ª" "ƒë·ªông")
                            pin_memory=pin_mem, num_workers=5,
                            worker_init_fn=worker_init_fn)
    
    val_loader = None
    if val_size > 0: 
        val_loader = DataLoader(TensorDataset(X_past_val, Y_future_val), 
                                batch_size=current_batch_size * 2, # (D√πng "m·∫ª" "ƒë·ªông" x2)
                                shuffle=False,
                                pin_memory=pin_mem, num_workers=2)
    
    # === (S·ª¨A V13: "D·ª±ng" "N√£o" TRANSFORMER) ===
    encoder = Encoder(lookback, lookforward, num_features, D_MODEL, N_HEAD, NUM_ENC_LAYERS, LATENT_DIM).to(device)
    
    # === ("V√Å" 1/2) ===
    decoder = Decoder(lookback, lookforward, num_features, D_MODEL, N_HEAD, NUM_ENC_LAYERS, NUM_DEC_LAYERS, LATENT_DIM).to(device)
    # ========================================
    
    monitor = HardwareMonitor(device.type) 
    
    params = list(encoder.parameters()) + list(decoder.parameters())
    optimizer = optim.Adam(params, lr=0.001)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_epochs, eta_min=1e-6)
    
    # === ("V√Å" L·ªñI 2/2) ===
    patience = 15; # <-- "Khai b√°o" (Define) `patience` (ch·ªØ th∆∞·ªùng)
    # =====================
    best_loss = float('inf'); patience_counter = 0; start_epoch = 0
    best_X_plot = None; best_Y_plot = None; best_attn_plot = None; best_epoch = 0
    
    if resume_file_name and os.path.exists(resume_file_name):
        logging.info(f"Ph√°t hi·ªán Checkpoint 'Luy·ªán Ti·∫øp'! ƒêang t·∫£i t·ª´: {resume_file_name}")
        try:
            checkpoint = torch.load(resume_file_name, map_location=device)
            encoder.load_state_dict(checkpoint['encoder_state_dict']) 
            decoder.load_state_dict(checkpoint['decoder_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            if 'scheduler_state_dict' in checkpoint:
                 scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            start_epoch = checkpoint['epoch'] + 1 
            best_loss = checkpoint['best_loss']
            patience_counter = checkpoint['patience_counter']
            logging.info(f"T·∫£i Checkpoint th√†nh c√¥ng! S·∫Ω 'luy·ªán ti·∫øp' t·ª´ Epoch {start_epoch}.")
        except Exception as e:
            logging.error(f"L·ªói t·∫£i Checkpoint: {e}. 'Luy·ªán' (Train) l·∫°i t·ª´ ƒë·∫ßu (Epoch 0).")
            start_epoch = 0

    try:
        for epoch in range(start_epoch, total_epochs):
            
            # --- (A) "LUY·ªÜN" (TRAIN) ---
            encoder.train(); decoder.train()
            total_train_loss = 0
            current_beta = BETA_VALUE * min(1.0, (epoch / BETA_ANNEAL_EPOCHS))
            pbar_desc = f"LB={lookback:03d} | E {epoch+1}/{total_epochs} [{mode_config['desc']}]"
            pbar_train = tqdm(train_loader, desc=pbar_desc, leave=False)
            
            for batch_X_past, batch_Y_future in pbar_train:
                batch_X_past_gpu = batch_X_past.to(device)
                batch_Y_future_gpu = batch_Y_future.to(device)
                batch_X_past_aug = augment_batch(batch_X_past_gpu)
                batch_Y_future_aug = augment_batch(batch_Y_future_gpu)
                z_mean, z_log_var = encoder(batch_X_past_aug, batch_Y_future_aug)
                z = sampling((z_mean, z_log_var))
                reconstruction, _ = decoder(batch_X_past_aug, z) 
                recon_loss_per_feature = torch.mean(
                    torch.square(batch_Y_future_aug - reconstruction) * weights_tensor, dim=2
                )
                recon_loss_per_sample = torch.sum(recon_loss_per_feature, dim=1)
                recon_loss = torch.mean(recon_loss_per_sample)
                kl_loss_per_sample = -0.5 * (1 + z_log_var - torch.square(z_mean) - torch.exp(z_log_var))
                kl_loss = torch.mean(torch.sum(kl_loss_per_sample, dim=1))
                total_loss = recon_loss + current_beta * kl_loss 
                optimizer.zero_grad()
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(params, max_norm=GRAD_CLIP_VALUE)
                optimizer.step()
                total_train_loss += total_loss.item()
                stats = monitor.get_stats()
                pbar_train.set_postfix(loss=total_loss.item(), **stats)
            
            avg_train_loss = total_train_loss / len(train_loader)
            
            # --- (B) "SOI" (VALIDATION) ---
            avg_val_loss = -1.0; avg_val_recon_loss = -1.0
            
            if val_split > 0.0 and val_loader:
                encoder.eval(); decoder.eval()
                total_val_loss = 0; total_val_recon_loss = 0 
                X_val_sample_plot_temp = None; Y_val_sample_plot_temp = None; attn_weights_plot_temp = None 
                
                with torch.no_grad():
                    for i_val, (batch_X_past_val, batch_Y_future_val) in enumerate(val_loader):
                        batch_X_past_val = batch_X_past_val.to(device)
                        batch_Y_future_val = batch_Y_future_val.to(device)
                        z_mean_val, z_log_var_val = encoder(batch_X_past_val, batch_Y_future_val)
                        z_val = sampling((z_mean_val, z_log_var_val))
                        reconstruction_val, attn_weights_val = decoder(batch_X_past_val, z_val)
                        recon_loss_val_feat = torch.mean(
                            torch.square(batch_Y_future_val - reconstruction_val) * weights_tensor, dim=2
                        )
                        recon_loss_val_samp = torch.sum(recon_loss_val_feat, dim=1)
                        recon_loss_val = torch.mean(recon_loss_val_samp)
                        kl_loss_val_samp = -0.5 * (1 + z_log_var_val - torch.square(z_mean_val) - torch.exp(z_log_var_val))
                        kl_loss_val = torch.mean(torch.sum(kl_loss_val_samp, dim=1))
                        total_val_loss += (recon_loss_val + current_beta * kl_loss_val).item()
                        total_val_recon_loss += recon_loss_val.item()
                        if i_val == 0:
                            X_val_sample_plot_temp = batch_X_past_val[0].cpu()
                            Y_val_sample_plot_temp = batch_Y_future_val[0].cpu()
                            # (V13 TCVAE kh√¥ng "nh·∫£" Attn)
                            # attn_weights_plot_temp = attn_weights_val[0].cpu().numpy() 

                avg_val_loss = total_val_loss / len(val_loader)
                avg_val_recon_loss = total_val_recon_loss / len(val_loader)
            
            # --- (C) LOGGING & CHECKPOINT ---
            if val_split > 0.0:
                logging.info(f"[LB={lookback:03d}] E{epoch+1:03d} | Beta: {current_beta:.4f} | Train Loss: {avg_train_loss:.4f} | **Val Loss: {avg_val_loss:.4f}** | Val Recon: {avg_val_recon_loss:.4f} | LR: {optimizer.param_groups[0]['lr']:.2e}")
            else:
                logging.info(f"[LB={lookback:03d}] E{epoch+1:03d} | Beta: {current_beta:.4f} | Train Loss (100%): {avg_train_loss:.4f} | LR: {optimizer.param_groups[0]['lr']:.2e}")
            
            scheduler.step()
            
            if writer:
                writer.add_scalar(f'Loss/Train_LB{lookback}', avg_train_loss, epoch)
                writer.add_scalar(f'Loss/Validation_LB{lookback}', avg_val_loss, epoch)
                writer.add_scalar(f'Loss/Val_Recon_Thu·∫ßn_LB{lookback}', avg_val_recon_loss, epoch)
                writer.add_scalar(f'Params/LearningRate_LB{lookback}', optimizer.param_groups[0]['lr'], epoch)
                writer.add_scalar(f'Params/Beta_LB{lookback}', current_beta, epoch)

            if use_patience:
                if avg_val_loss < best_loss:
                    best_loss = avg_val_loss; patience_counter = 0
                    torch.save(decoder.state_dict(), final_model_name) 
                    logging.info(f"-> [LB={lookback}] ƒê√£ l∆∞u 'N√£o V·∫Ω Transformer' (Val Loss: {best_loss:.4f}) -> {final_model_name}")
                    best_X_plot = X_val_sample_plot_temp; best_Y_plot = Y_val_sample_plot_temp
                    # best_attn_plot = attn_weights_plot_temp # (V13 kh√¥ng "v·∫Ω" Attn)
                    best_epoch = epoch
                else:
                    patience_counter += 1
                if resume_file_name:
                    torch.save({ 'epoch': epoch, 'encoder_state_dict': encoder.state_dict(), 
                        'decoder_state_dict': decoder.state_dict(), 'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(), 'best_loss': best_loss,
                        'patience_counter': patience_counter }, resume_file_name)
                
                # === ("V√Å" L·ªñI 2/2) ===
                if patience_counter >= patience: 
                # =====================
                    logging.info(f"[LB={lookback}] D·ª´ng s·ªõm (EarlyStopping) t·∫°i Epoch {epoch+1}.")
                    break
    
    except KeyboardInterrupt:
        logging.warning(f"\n[LB={lookback}] ƒê√£ b·∫Øt ƒë∆∞·ª£c (Ctrl+C)! ƒêang 'l∆∞u kh·∫©n c·∫•p'...")
        torch.save(decoder.state_dict(), final_model_name + ".interrupt.pth")
        if pynvml: pynvml.nvmlShutdown() 
        sys.exit(0) 
    
    # --- (D) "TR·∫¢ H√ÄNG" (V·∫º ·∫¢NH) ---
    if not use_patience:
        torch.save(decoder.state_dict(), final_model_name)
        logging.info(f"-> 'N√£o X·ªãn' 100% (Epoch {total_epochs}) ƒë√£ l∆∞u -> {final_model_name}")
    else:
        try:
            decoder.load_state_dict(torch.load(final_model_name))
            logging.info(f"ƒêang 'V·∫Ω' 1 Chart (PNG) 'X·ªãn' nh·∫•t (t·ª´ Epoch {best_epoch})...")
            if best_X_plot is not None:
                plot_cvae_samples(best_X_plot, best_Y_plot, encoder, decoder, writer, best_epoch, lookback)
                # plot_attention_heatmap(best_attn_plot, writer, best_epoch, lookback) # (V13 kh√¥ng "v·∫Ω" Attn)
            else:
                logging.warning("Kh√¥ng t√¨m th·∫•y 'h√†ng' (data) 'x·ªãn' nh·∫•t ƒë·ªÉ 'v·∫Ω' chart PNG.")
        except FileNotFoundError:
             logging.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y file 'n√£o x·ªãn' {final_model_name} ƒë·ªÉ 'v·∫Ω'.")
        
    logging.info(f"=== HO√ÄN T·∫§T 'D√ÇY CHUY·ªÄN' (Lookback={lookback}) ===")
    if writer: writer.close() 

    # === ("V√Å" L·ªñI 4 - "TR·∫¢ H√ÄNG" VRAM OOM) ===
    return (encoder, decoder, X_past_t, Y_future_t, X_past_train, Y_future_train)
    # =========================================

# --- (H√ÄM MAIN "C√îNG X∆Ø·ªûNG" V13 - "ƒÇn" 53 M√≥n) ---
if __name__ == "__main__":

    print("\n" + "="*70)
    print("      L√í ƒê√öC N√ÉO TCVAE V1 ('N√£o' TRANSFORMER 'ƒÇn' 53 M√≥n)")
    print("="*70)
    
    logging.info("--- CH·∫æ ƒê·ªò 'SOI H√ÄNG' (V13) ƒêANG KH·ªûI ƒê·ªòNG (Luy·ªán 80/20) ---")
    
    mode_config = {
        'desc': "SOI V13 80/20 ('N√£o' Transformer)",
        'epochs': DEFAULT_SOI_EPOCHS, 
        'val_split': 0.2, 
        'log_dir': DEFAULT_TENSORBOARD_DIR,
        'final_name': os.path.join(DIR_MODELS, "transformer_cvae_decoder_V13_{lb}_{lf}_best.pth"),
        'resume_name': os.path.join(DIR_MODELS, "transformer_cvae_model_V13_{lb}_resume.pth"),
        'use_patience': True, 
        'log_file': "log_train_transformer_cvae_V13_SOI.log"
    }

    # --- "L·∫ÆP" ƒê·ªí NGH·ªÄ ---
    try: import psutil
    except ImportError: psutil = None
        
    log_filename = mode_config['log_file']
    logging.basicConfig(level=logging.INFO, 
                        format='%(asctime)s - %(levelname)s - [TrainTCVAE_V13] - %(message)s',
                        handlers=[
                            logging.FileHandler(log_filename, mode='w', encoding='utf-8'), 
                            logging.StreamHandler(sys.stdout) 
                        ],
                        force=True) 
                        
    logging.info(f"Log s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o file: {log_filename}")
    logging.info(f"ƒêang s·ª≠ d·ª•ng thi·∫øt b·ªã: {device}")
    
    try:
        import pynvml 
        pynvml.nvmlInit()
        GPU_HANDLE = pynvml.nvmlDeviceGetHandleByIndex(0)
    except Exception:
        try:
            import nvidia_ml as pynvml 
            pynvml.nvmlInit()
            GPU_HANDLE = pynvml.nvmlDeviceGetHandleByIndex(0)
        except Exception: pynvml = None 
    
    os.makedirs(DIR_MODELS, exist_ok=True)
    os.makedirs(DIR_PROCESSED, exist_ok=True)
    if mode_config['log_dir']: os.makedirs(mode_config['log_dir'], exist_ok=True) 
    
    logging.info(f"=== 'L√í' V13 (TCVAE) CH·∫†Y CH·∫æ ƒê·ªò: {mode_config['desc']} ===")
            
    # 1. T·∫¢I "TH·ª®C ƒÇN" V√Ä "ƒÇN K√â" SCALER V23
    data_scaled, num_features, feature_names = load_data_and_scaler_V23()
    
    if data_scaled is None:
        logging.error("D·ª´ng 'C√îNG X∆Ø·ªûNG' (TCVAE V13) v√¨ kh√¥ng c√≥ 'th·ª©c ƒÉn' ho·∫∑c 'scaler'.")
        if pynvml: pynvml.nvmlShutdown() 
        sys.exit(1)
        
    # 2. "CH·∫æ" LOSS "QUY·ªÄN TR·ªåNG" (Weighted Loss)
    logging.info(f"ƒêang 'ch·∫ø' Weighted Loss V13 (x{WEIGHTED_LOSS_MULTIPLIER} cho OHLC)...")
    try:
        weights_tensor = torch.ones(num_features).to(device)
        
        vip_cols = ['H1_Close', 'H1_High', 'H1_Low', 'H1_Open']
        found_count = 0
        for col in vip_cols:
            try:
                # (D√πng feature_names (V13) tr·∫£ v·ªÅ t·ª´ h√†m load_data)
                idx = feature_names.index(col)
                weights_tensor[idx] = WEIGHTED_LOSS_MULTIPLIER
                
                if col == 'H1_Close':
                    CVAE_CLOSE_IDX_FOR_PLOT = idx
                    # "M√≥c" th√¥ng s·ªë unscale t·ª´ scaler V23
                    scaler_v23 = joblib.load(SCALER_FILENAME)
                    SCALER_MIN_CLOSE = scaler_v23.min_[idx]
                    SCALER_SCALE_CLOSE = scaler_v23.scale_[idx]
                    
                found_count += 1
            except (ValueError, IndexError):
                logging.warning(f"Kh√¥ng t√¨m th·∫•y c·ªôt '{col}' trong 'th·ª©c ƒÉn 53 m√≥n' ƒë·ªÉ 'ƒë·ªô' Weighted Loss.")
        
        if CVAE_CLOSE_IDX_FOR_PLOT == -1:
            logging.error("L·ªñI CH√ç M·∫†NG: Kh√¥ng t√¨m th·∫•y 'H1_Close' trong 'th·ª©c ƒÉn 53 m√≥n'. Kh√¥ng 'v·∫Ω' ƒë∆∞·ª£c.")
            sys.exit(1)
            
        logging.info(f"ƒê√£ 'ƒë·ªô' Weighted Loss (x{WEIGHTED_LOSS_MULTIPLIER}) cho {found_count} c·ªôt 'VIP'.")
    except Exception as e:
        logging.error(f"L·ªñI 'ƒë·ªô' Weighted Loss. D√πng t·∫°m loss 'c√†o b·∫±ng'. L·ªói: {e}")
        weights_tensor = torch.ones(num_features).to(device)
    
    # 3. L·∫∂P "ƒê√öC" 2 N√ÉO (50, 168)
    ALL_LOOKBACKS = [50, 168]
    
    # (Bi·∫øn "to√†n c·ª•c" ƒë·ªÉ "Nuke" VRAM)
    encoder = None
    decoder = None
    X_past_t = None
    Y_future_t = None
    X_past_train = None
    Y_future_train = None
    
    for lb in ALL_LOOKBACKS:
        logging.info(f"\n{'='*70}\n === B·∫ÆT ƒê·∫¶U 'D√ÇY CHUY·ªÄN' (TCVAE V13 'N√£o M·ªõi' Lookback={lb}) ===\n{'='*70}")
        
        X_past, Y_future = create_windows(data_scaled, lb, LOOKFORWARD)
        
        if X_past is None:
            logging.warning(f"B·ªè qua Lookback={lb} do kh√¥ng ƒë·ªß d·ªØ li·ªáu 'c·∫Øt'.")
            continue
            
        # (S·ª¨A V13 - V√Å L·ªñI OOM)
        # "Nuke" (gi·∫øt) 2 "n√£o" v√† data "c≈©" (n·∫øu c√≥)
        if 'encoder' in locals() and encoder is not None: del encoder
        if 'decoder' in locals() and decoder is not None: del decoder
        if 'X_past_t' in locals() and X_past_t is not None: del X_past_t
        if 'Y_future_t' in locals() and Y_future_t is not None: del Y_future_t
        if 'X_past_train' in locals() and X_past_train is not None: del X_past_train
        if 'Y_future_train' in locals() and Y_future_train is not None: del Y_future_train
        
        # "D·ªçn" (empty) VRAM
        torch.cuda.empty_cache() 
        logging.info(f"ƒê√£ 'Nuke' VRAM. B·∫Øt ƒë·∫ßu 'luy·ªán' Lookback={lb}...")
        
        (encoder, decoder, X_past_t, Y_future_t, X_past_train, Y_future_train) = train_single_cvae_pytorch(
            X_past, Y_future, 
            lb, LOOKFORWARD, num_features, 
            weights_tensor,
            mode_config 
        )
    
    logging.info(f"\n{'='*70}\n === HO√ÄN T·∫§T 'L√í V13' (TCVAE) - ƒê√É CH·∫†Y XONG {len(ALL_LOOKBACKS)} N√ÉO! ===\n{'='*70}")
    
    logging.info(f"ƒê√£ 'xu·∫•t' ·∫£nh PNG 'X·ªãn' nh·∫•t (TCVAE V13) v√†o th∆∞ m·ª•c: {DIR_MODELS}")
    logging.info(f"ƒê·∫°i ca 'soi' (V13) b·∫±ng: tensorboard --logdir={mode_config['log_dir']}")
    
    if pynvml:
        pynvml.nvmlShutdown()