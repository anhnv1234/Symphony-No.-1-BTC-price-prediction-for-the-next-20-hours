import pandas as pd
import numpy as np
import os
import logging
import joblib
from sklearn.preprocessing import MinMaxScaler
import warnings 
import psutil 
import sys 
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from tqdm import tqdm

# --- KH·∫ÆC PH·ª§C L·ªñI HI·ªÇN TH·ªä TI·∫æNG VI·ªÜT TR√äN WINDOWS ---
if sys.version_info.major == 3 and sys.version_info.minor >= 7:
    try:
        sys.stdout.reconfigure(encoding='utf-8')
    except AttributeError:
        pass

# --- TH·∫¶N CH√ö PYTORCH ---
warnings.filterwarnings("ignore", category=FutureWarning, module="torch.cuda")
warnings.filterwarnings("ignore", category=UserWarning, module="torch.utils.data")

# =========================================================================
# üí° KI·∫æN TR√öC ADVANCED TIME-SERIES GAN (D√ôNG GRU V√Ä LATENT SPACE)
# =========================================================================

# --- M√¥ h√¨nh GRU c∆° s·ªü ---
class BaseGRU(nn.Module):
    """L·ªõp GRU c∆° s·ªü cho c√°c m·∫°ng E, R, G, D"""
    def __init__(self, input_dim, output_dim, hidden_dim, num_layers=2):
        super().__init__()
        self.rnn = nn.GRU(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True
        )
        self.output_layer = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        # output: (batch_size, seq_len, hidden_dim)
        output, _ = self.rnn(x) 
        # Tr·∫£ v·ªÅ k·∫øt qu·∫£ sau khi qua l·ªõp tuy·∫øn t√≠nh
        return self.output_layer(output)

# 1. Embedder (E): X -> H (Th·ª±c)
class AdvancedTS_Embedder(BaseGRU):
    def __init__(self, num_features, hidden_dim):
        super().__init__(num_features, hidden_dim, hidden_dim)

# 2. Recovery (R): H -> X (Th·ª±c)
class AdvancedTS_Recovery(BaseGRU):
    def __init__(self, num_features, hidden_dim):
        super().__init__(hidden_dim, num_features, hidden_dim)

# 3. Generator (G): Z -> H_tilde (Gi·∫£ trong Latent Space)
class AdvancedTS_Generator(BaseGRU):
    def __init__(self, hidden_dim):
        super().__init__(hidden_dim, hidden_dim, hidden_dim)

# 4. Discriminator (D): H/H_tilde -> Score (Latent Space Discrimination)
class AdvancedTS_Discriminator(BaseGRU):
    def __init__(self, hidden_dim):
        # ƒê·∫ßu ra 1 scalar (score) cho m·ªói b∆∞·ªõc th·ªùi gian
        super().__init__(hidden_dim, 1, hidden_dim)


class AdvancedTS_Trainer:
    """Class qu·∫£n l√Ω qu√° tr√¨nh hu·∫•n luy·ªán 3 b∆∞·ªõc: Reconstruction, Supervised, Adversarial"""
    def __init__(self, lookback, num_features, hidden_dim, device):
        self.device = device
        self.lookback = lookback
        self.hidden_dim = hidden_dim
        
        # Kh·ªüi t·∫°o 4 m·∫°ng c·ªët l√µi
        self.E = AdvancedTS_Embedder(num_features, hidden_dim).to(device)
        self.R = AdvancedTS_Recovery(num_features, hidden_dim).to(device)
        self.G = AdvancedTS_Generator(hidden_dim).to(device)
        self.D = AdvancedTS_Discriminator(hidden_dim).to(device)

        # Optimizers (S·ª≠ d·ª•ng c√°c optimizer ri√™ng bi·ªát cho t·ª´ng nh√≥m Loss)
        self.lr = 1e-3
        self.optimizer_ER = optim.Adam(list(self.E.parameters()) + list(self.R.parameters()), lr=self.lr)
        self.optimizer_G = optim.Adam(self.G.parameters(), lr=self.lr)
        self.optimizer_D = optim.Adam(self.D.parameters(), lr=self.lr)

        # H√†m m·∫•t m√°t
        self.mse = nn.MSELoss()
        self.bce = nn.BCEWithLogitsLoss() # T·ªët h∆°n cho GAN
        self.lambda_adv = 1.0 # Tr·ªçng s·ªë cho Adversarial Loss
        self.lambda_sup = 1.0 # Tr·ªçng s·ªë cho Supervised Loss
        self.lambda_rec = 10.0 # Tr·ªçng s·ªë l·ªõn h∆°n cho Reconstruction (Consistency) Loss

    def _get_noise(self, batch_size):
        """T·∫°o vector nhi·ªÖu Z c√≥ c√πng shape v·ªõi d·ªØ li·ªáu"""
        return torch.randn(batch_size, self.lookback, self.hidden_dim).to(self.device)

    # =====================================================
    # 1. HU·∫§N LUY·ªÜN T√ÅI T·∫†O (RECONSTRUCTION WARM-UP)
    # =====================================================
    def train_step_reconstruction(self, real_data_batch):
        self.optimizer_ER.zero_grad()
        
        H_real = self.E(real_data_batch) # X -> H
        X_reconstructed = self.R(H_real) # H -> X_reconstructed
        
        # Loss T√°i t·∫°o: E(R(X)) n√™n gi·ªëng X
        loss_R = self.mse(real_data_batch, X_reconstructed)
        
        loss_R.backward()
        self.optimizer_ER.step()
        return loss_R.item()

    # =====================================================
    # 2. HU·∫§N LUY·ªÜN ADVERSARIAL V√Ä SUPERVISED (SUPERIOR TRAINING)
    # C√ì B·ªî SUNG AUTOREGRESSIVE V√Ä CONSISTENCY LOSS
    # =====================================================
    def train_step_adversarial(self, real_data_batch):
        batch_size = real_data_batch.size(0)
        
        # --- A. C·∫≠p nh·∫≠t Generator (G) v√† Embedder/Recovery (E/R) ---
        # M·ª•c ti√™u: ƒê·∫£m b·∫£o G t·∫°o ra d·ªØ li·ªáu c√≥ th·ªÉ ph·ª•c h·ªìi t·ªët v√† tu√¢n theo dynamics (S + R)
        self.optimizer_G.zero_grad()
        self.optimizer_ER.zero_grad() 
        
        # L·∫•y Embedded sequences th·ª±c
        H_real = self.E(real_data_batch) 
        
        # 1. Unsupervised Adversarial Loss (Loss U)
        Z = self._get_noise(batch_size)
        H_synthetic = self.G(Z)
        D_synthetic_for_G = self.D(H_synthetic)
        # G mu·ªën D_synthetic -> 1
        loss_U = self.bce(D_synthetic_for_G, torch.ones_like(D_synthetic_for_G))
        
        # 2. Supervised Loss (Loss S - Autoregressive Proxy)
        # Khuy·∫øn kh√≠ch G h·ªçc √°nh x·∫° ƒë·ªông l·ª±c c·ªßa d·ªØ li·ªáu th·ª±c
        H_predicted = self.G(H_real) 
        loss_S = self.mse(H_predicted, H_real)
        
        # 3. Reconstruction/Consistency Loss (Loss R)
        # ƒê·∫£m b·∫£o E v√† R v·∫´n ho·∫°t ƒë·ªông t·ªët, v√† G t·∫°o ra latent code c√≥ th·ªÉ recover
        X_reconstructed = self.R(H_real) # R(E(X))
        loss_R = self.mse(real_data_batch, X_reconstructed)

        # T·ªïng Loss G (U + S + R)
        loss_G_final = self.lambda_adv * loss_U + self.lambda_sup * loss_S + self.lambda_rec * torch.sqrt(loss_R)
        
        loss_G_final.backward(retain_graph=True) # Retain graph c·∫ßn thi·∫øt v√¨ D s·∫Ω d√πng E/G/H
        self.optimizer_G.step()
        # E, R ƒë∆∞·ª£c t·ªëi ∆∞u c√πng G (qua Loss R)
        self.optimizer_ER.step()
        
        # --- B. C·∫≠p nh·∫≠t Discriminator (D) ---
        self.optimizer_D.zero_grad()
        
        # D-Loss tr√™n d·ªØ li·ªáu TH·∫¨T (target=1)
        # H_real ph·∫£i ƒë∆∞·ª£c t√≠nh l·∫°i m√† kh√¥ng qua gradient c·ªßa G
        H_real = self.E(real_data_batch).detach() 
        D_real = self.D(H_real)
        loss_D_real = self.bce(D_real, torch.ones_like(D_real))
        
        # D-Loss tr√™n d·ªØ li·ªáu GI·∫¢ (target=0)
        # H_synthetic ph·∫£i ƒë∆∞·ª£c t√≠nh l·∫°i m√† kh√¥ng qua gradient c·ªßa G
        Z = self._get_noise(batch_size)
        H_synthetic = self.G(Z).detach()
        D_synthetic = self.D(H_synthetic)
        loss_D_fake = self.bce(D_synthetic, torch.zeros_like(D_synthetic))
        
        # D-Loss t·ªïng
        loss_D = loss_D_real + loss_D_fake
        loss_D_final = loss_D * self.lambda_adv
        
        loss_D_final.backward()
        self.optimizer_D.step()
        
        return loss_G_final.item(), loss_D_final.item() #, loss_R.item() # C√≥ th·ªÉ tr·∫£ v·ªÅ Loss R ƒë·ªÉ theo d√µi

    def get_all_states(self):
        """L·∫•y t·∫•t c·∫£ state dict c·∫ßn thi·∫øt cho vi·ªác l∆∞u tr·ªØ"""
        return {
            'E_state_dict': self.E.state_dict(),
            'R_state_dict': self.R.state_dict(),
            'G_state_dict': self.G.state_dict(),
            'D_state_dict': self.D.state_dict(),
            'optER_state_dict': self.optimizer_ER.state_dict(),
            'optG_state_dict': self.optimizer_G.state_dict(),
            'optD_state_dict': self.optimizer_D.state_dict(),
        }

# =========================================================================

# --- KH·ªûI T·∫†O V√Ä C·∫§U H√åNH ---

pynvml = None
GPU_HANDLE = None

# C·ªë g·∫Øng import pynvml ƒë·ªÉ theo d√µi GPU
try:
    import pynvml
    pynvml.nvmlInit()
except (ImportError, Exception):
    pynvml = None
    if torch.cuda.is_available():
        logging.warning("Kh√¥ng t√¨m th·∫•y 'linh ki·ªán' pynvml. S·∫Ω kh√¥ng theo d√µi ƒë∆∞·ª£c tr·∫°ng th√°i GPU.")
    
# C·∫•u h√¨nh logging (Th√™m FileHandler v·ªõi encoding UTF-8)
log_filename = "log_train_advanced_tsgan.log" 
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - [AdvancedTSGAN_PYTORCH] - %(message)s',
                    handlers=[
                        logging.FileHandler(log_filename, mode='a', encoding='utf-8'), 
                        logging.StreamHandler()
                    ])
logging.info(f"Log s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o file: {log_filename}")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logging.info(f"ƒêang s·ª≠ d·ª•ng thi·∫øt b·ªã: {device}")

MASTER_FILE_PATH = os.path.join('02_Master_Data', 'btcusdt_master_data.parquet')
SCALER_FILENAME = os.path.join('01_Processed_Data', 'cvae_scaler_V23.gz')

# Thay ƒë·ªïi s·ªë b∆∞·ªõc hu·∫•n luy·ªán ƒë·ªÉ d√†nh cho t·ª´ng giai ƒëo·∫°n
WARMUP_STEPS = 500 # B∆∞·ªõc kh·ªüi ƒë·ªông (Reconstruction)
ADVERSARIAL_STEPS = 4500 # B∆∞·ªõc ch√≠nh (Adversarial + Supervised + Consistency)
TRAIN_STEPS = WARMUP_STEPS + ADVERSARIAL_STEPS

GAN_BATCH_SIZE = 64
HIDDEN_DIM = 24 

DIR_MODELS = "03_Models"
DIR_PROCESSED = "01_Processed_Data"
os.makedirs(DIR_MODELS, exist_ok=True)
os.makedirs(DIR_PROCESSED, exist_ok=True)


class HardwareMonitor:
    # L·ªõp theo d√µi ph·∫ßn c·ª©ng (gi·ªØ nguy√™n)
    def __init__(self, device):
        self.device = device
        self.pynvml = None
        self.gpu_handle = None
        self.nvml_imported = False
        
        if self.device.type == 'cuda':
            try:
                global pynvml
                if pynvml:
                    self.pynvml = pynvml
                    self.gpu_handle = self.pynvml.nvmlDeviceGetHandleByIndex(0)
                    self.nvml_imported = True
            except Exception:
                self.nvml_imported = False

    def log_usage(self, prefix=""):
        if self.device.type == 'cuda' and self.nvml_imported:
            try:
                info = self.pynvml.nvmlDeviceGetMemoryInfo(self.gpu_handle)
                gpu_memory = f"VRAM: {info.used / 1024**3:.2f} GB / {info.total / 1024**3:.2f} GB | "
            except Exception:
                gpu_memory = "VRAM: N/A | "
        else:
            gpu_memory = ""

        process = psutil.Process(os.getpid())
        mem_info = process.memory_info()
        ram_usage = f"RAM: {mem_info.rss / 1024**3:.2f} GB"
        
        logging.info(f"{prefix}{gpu_memory}{ram_usage}")


# --- H√ÄM LOAD DATA (Gi·ªØ nguy√™n) ---
def load_data_and_scaler_V23():
    logging.info(f"ƒêang t·∫£i file master V23 (53 m√≥n): {MASTER_FILE_PATH}...")
    try:
        df = pd.read_parquet(MASTER_FILE_PATH)
    except FileNotFoundError:
        logging.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y file {MASTER_FILE_PATH}.")
        return None, None
        
    logging.info(f"ƒêang t·∫£i 'B·ªô Chu·∫©n H√≥a' (V23 - 53 m√≥n) (d√πng chung CVAE): {SCALER_FILENAME}...")
    try:
        scaler = joblib.load(SCALER_FILENAME)
    except FileNotFoundError:
        logging.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y file {SCALER_FILENAME}.")
        logging.error("ƒê·∫°i ca ƒë√£ ch·∫°y 'L√≤' (train_cvae_V11.py) (file 'ƒÉn' 53 m√≥n) ch∆∞a?")
        return None, None
    
    df.interpolate(method='time', inplace=True)
    df.fillna(method='ffill', inplace=True)
    df.fillna(method='bfill', inplace=True)
    df.fillna(0, inplace=True) # (V√° V5: "Tr√°m" 0 (n·∫øu ffill/bfill v·∫´n s√≥t))

    features_df = df
    feature_names = features_df.columns.tolist()
    num_features = len(feature_names)
    logging.info(f"T·ªïng s·ªë features (m√≥n ƒÉn V23 - 53 m√≥n) ƒëang s·ª≠ d·ª•ng: {num_features}")

    logging.info("ƒêang chu·∫©n h√≥a 'th·ª©c ƒÉn' (d√πng Scaler L√≤ V11)...")
    data_scaled = scaler.transform(features_df)
    
    return data_scaled, num_features


# --- H√ÄM CREATE WINDOWS (Gi·ªØ nguy√™n) ---
def create_windows_for_seriesgan(data_scaled, lookback):
    logging.info(f"ƒêang 'c·∫Øt' c·ª≠a s·ªï AdvancedTSGAN (Lookback={lookback})...")
    
    num_samples = len(data_scaled) - lookback + 1
    
    if num_samples <= 0:
        logging.error(f"L·ªñI (Lookback={lookback}): D·ªØ li·ªáu qu√° √≠t, kh√¥ng ƒë·ªß 'c·∫Øt' 1 c·ª≠a s·ªï.")
        return None, None

    # D√πng 'stride_tricks' ƒë·ªÉ "c·∫Øt" c·ª≠a s·ªï (si√™u nhanh)
    n_features = data_scaled.shape[1]
    shape = (num_samples, lookback, n_features)
    strides = (data_scaled.strides[0], data_scaled.strides[0], data_scaled.strides[1])
    X_tsgan_np = np.lib.stride_tricks.as_strided(data_scaled, shape=shape, strides=strides)
    
    logging.info(f"Shape 'Th·ª©c ƒÇn' AdvancedTSGAN (NP): {X_tsgan_np.shape}") 
    
    # Chuy·ªÉn sang PyTorch Tensor
    X_tsgan_tensor = torch.tensor(X_tsgan_np, dtype=torch.float32)
    
    # T·∫°o Dataset v√† DataLoader
    dataset = TensorDataset(X_tsgan_tensor)
    dataloader = DataLoader(dataset, batch_size=GAN_BATCH_SIZE, shuffle=True, drop_last=True)
    
    return X_tsgan_tensor, dataloader


def train_single_seriesgan_pytorch(dataloader, lookback, num_features):
    """Hu·∫•n luy·ªán Advanced Time-series GAN qua 3 giai ƒëo·∫°n: Reconstruction, Supervised, Adversarial"""
    
    monitor = HardwareMonitor(device)
    monitor.log_usage(prefix="[TR∆Ø·ªöC TRAIN] ")

    # 1. LOGIC "L∆ØU/T·∫¢I" CHECKPOINT
    checkpoint_resume_path = os.path.join(DIR_MODELS, f'advanced_tsgan_checkpoint_{lookback}_resume.pth')
    
    # 2. KH·ªûI T·∫†O M√î H√åNH
    try:
        synthesizer = AdvancedTS_Trainer(lookback=lookback, num_features=num_features, hidden_dim=HIDDEN_DIM, device=device)
    except Exception as e:
        logging.error(f"L·ªói kh·ªüi t·∫°o AdvancedTS_Trainer: {e}")
        return

    start_step = 0
    
    if os.path.exists(checkpoint_resume_path):
        logging.info(f"Ph√°t hi·ªán Checkpoint 'Luy·ªán Ti·∫øp'! ƒêang t·∫£i t·ª´: {checkpoint_resume_path}")
        try:
            checkpoint = torch.load(checkpoint_resume_path, map_location=device)
            synthesizer.E.load_state_dict(checkpoint['E_state_dict']) 
            synthesizer.R.load_state_dict(checkpoint['R_state_dict']) 
            synthesizer.G.load_state_dict(checkpoint['G_state_dict']) 
            synthesizer.D.load_state_dict(checkpoint['D_state_dict']) 
            synthesizer.optimizer_ER.load_state_dict(checkpoint['optER_state_dict'])
            synthesizer.optimizer_G.load_state_dict(checkpoint['optG_state_dict'])
            synthesizer.optimizer_D.load_state_dict(checkpoint['optD_state_dict'])
            start_step = checkpoint['step']
            logging.info(f"T·∫£i Checkpoint th√†nh c√¥ng! S·∫Ω 'luy·ªán ti·∫øp' t·ª´ Step: {start_step}")
        except Exception as e:
            logging.error(f"L·ªói t·∫£i Checkpoint: {e}. 'Luy·ªán' (Train) l·∫°i t·ª´ ƒë·∫ßu.")
            start_step = 0
            
    # 3. HU·∫§N LUY·ªÜN (V√≤ng l·∫∑p ti√™u chu·∫©n PyTorch)
    logging.info(f"B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán [AdvancedTSGAN Lookback={lookback}]...")
    logging.info(f"(Warmup: {WARMUP_STEPS} steps, Adversarial: {ADVERSARIAL_STEPS} steps) - T·ªïng: {TRAIN_STEPS} m·∫ª...")
    
    current_stage = "Reconstruction Warmup"
    
    try:
        data_iterator = iter(dataloader)
        
        for step in tqdm(range(start_step, TRAIN_STEPS), initial=start_step, total=TRAIN_STEPS, desc=f"LB={lookback}"):
            
            # Chuy·ªÉn sang giai ƒëo·∫°n Adversarial
            if step == WARMUP_STEPS:
                current_stage = "Adversarial Training"
                logging.info(f"\n[Step {step}] === CHUY·ªÇN SANG GIAI ƒêO·∫†N HU·∫§N LUY·ªÜN ƒê·ªêI NGH·ªäCH (ADVERSARIAL + SUPERVISED + CONSISTENCY) ===")

            try:
                real_data_batch = next(data_iterator)[0].to(device)
            except StopIteration:
                data_iterator = iter(dataloader)
                real_data_batch = next(data_iterator)[0].to(device)
            
            # --- Th·ª±c hi·ªán c√°c b∆∞·ªõc hu·∫•n luy·ªán theo giai ƒëo·∫°n ---
            
            loss_G, loss_D, loss_R_cont = None, None, None
            
            if step < WARMUP_STEPS:
                # Giai ƒëo·∫°n 1: Ch·ªâ hu·∫•n luy·ªán E v√† R (T√°i t·∫°o)
                loss_R_cont = synthesizer.train_step_reconstruction(real_data_batch) 
            else:
                # Giai ƒëo·∫°n 2: Hu·∫•n luy·ªán G, D, v√† E/R li√™n t·ª•c
                loss_G, loss_D = synthesizer.train_step_adversarial(real_data_batch)
                # L·∫•y loss R ri√™ng ƒë·ªÉ log
                H_real_temp = synthesizer.E(real_data_batch).detach()
                X_reconstructed_temp = synthesizer.R(H_real_temp)
                loss_R_cont = synthesizer.mse(real_data_batch, X_reconstructed_temp).item()

            
            # Th√™m log ƒë·ªãnh k·ª≥ ƒë·ªÉ theo d√µi
            if step > start_step and step % 500 == 0:
                 log_msg = f"[{current_stage} Step {step}]"
                 if loss_R_cont is not None: log_msg += f" Loss R: {loss_R_cont:.4f}"
                 if loss_G is not None: log_msg += f" Loss G: {loss_G:.4f}, Loss D: {loss_D:.4f}"
                 logging.info(log_msg)

        
        # 4. L∆ØU "N√£o x·ªãn" (Cu·ªëi c√πng)
        model_final_path = os.path.join(DIR_MODELS, f'advanced_tsgan_model_{lookback}_final.pth') 
        
        final_states = synthesizer.get_all_states()
        final_states['step'] = TRAIN_STEPS 
        
        torch.save(final_states, model_final_path)
        
        logging.info(f"L∆∞u 'N√£o x·ªãn' (Final) th√†nh c√¥ng -> {model_final_path}")
        
        if os.path.exists(checkpoint_resume_path):
            os.remove(checkpoint_resume_path)
            
        logging.info(f"=== HO√ÄN T·∫§T L√í 2.C (AdvancedTSGAN - Lookback={lookback}) ===")
    
    except KeyboardInterrupt:
        # B·∫Øt "Ctrl+C" v√† L∆∞u Checkpoint Kh·∫©n c·∫•p
        logging.warning(f"\n[LB={lookback}] ƒê√£ b·∫Øt ƒë∆∞·ª£c (Ctrl+C)! ƒêang 'l∆∞u kh·∫©n c·∫•p' Checkpoint 'Luy·ªán Ti·∫øp'...")
        # L∆ØU KH·∫®N C·∫§P
        torch.save({
            'step': step,
            **synthesizer.get_all_states()
        }, checkpoint_resume_path)
        logging.info(f"L∆∞u kh·∫©n c·∫•p th√†nh c√¥ng -> {checkpoint_resume_path}. L·∫ßn sau ch·∫°y l·∫°i s·∫Ω 'luy·ªán ti·∫øp' t·ª´ ƒë√¢y.")
        if pynvml: pynvml.nvmlShutdown() 
        sys.exit(0) 
        
    except Exception as e:
        logging.error(f"L·ªñI CH√ç M·∫†NG khi 'luy·ªán' AdvancedTSGAN (Lookback={lookback}): {e}")
        # C·ªë g·∫Øng l∆∞u kh·∫©n c·∫•p l·∫ßn cu·ªëi
        if 'synthesizer' in locals():
            last_states = synthesizer.get_all_states()
            last_states['step'] = step 
            torch.save(last_states, checkpoint_resume_path) 
        return 


# --- H√ÄM MAIN "C√îNG X∆Ø·ªûNG" ---
if __name__ == "__main__":
    
    logging.info(f"=== KH·ªûI ƒê·ªòNG 'C√îNG X∆Ø·ªûNG' ƒê√öC N√ÉO ADVANCED TS-GAN (PYTORCH - 53 M√ìN) ===")
    
    # 1. T·∫¢I V√Ä X·ª¨ L√ù "TH·ª®C ƒÇN"
    data_scaled, num_features = load_data_and_scaler_V23()
    
    if data_scaled is None:
        logging.error("D·ª´ng 'C√îNG X∆Ø·ªûNG' (AdvancedTSGAN) v√¨ kh√¥ng c√≥ 'th·ª©c ƒÉn' ho·∫∑c 'scaler'.")
        if pynvml: 
            try: pynvml.nvmlShutdown()
            except: pass
        sys.exit(1)
        
    # 2. L·∫∂P "ƒê√öC" N√ÉO 
    ALL_LOOKBACKS = [50, 168]
    
    for lb in ALL_LOOKBACKS:
        logging.info(f"\n{'='*70}\n === B·∫ÆT ƒê·∫¶U 'D√ÇY CHUY·ªÄN' (AdvancedTSGAN Lookback={lb}) ===\n{'='*70}")
        
        # 3. "C·∫ÆT" C·ª¨A S·ªî (Tr·∫£ v·ªÅ DataLoader)
        X_tsgan_tensor, dataloader = create_windows_for_seriesgan(data_scaled, lb)
        
        if dataloader is None:
            logging.warning(f"B·ªè qua Lookback={lb} do kh√¥ng ƒë·ªß d·ªØ li·ªáu 'c·∫Øt'.")
            continue
            
        # 4. "ƒê√öC" (Train)
        train_single_seriesgan_pytorch(dataloader, lb, num_features)
        
        best_model_path_for_this_lb = os.path.join(DIR_MODELS, f'advanced_tsgan_model_{lb}_final.pth')
        logging.info(f"\n{'*' * 25} HO√ÄN TH√ÄNH 'ƒê√öC' N√ÉO (AdvancedTSGAN Lookback={lb}) {'*' * 25}")
        logging.info(f"-> 'N√£o x·ªãn' nh·∫•t (final model) ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {best_model_path_for_this_lb}")
        logging.info(f"ƒêang d·ªçn d·∫πp VRAM (empty_cache) tr∆∞·ªõc khi sang 'd√¢y chuy·ªÅn' Lookback ti·∫øp theo...")
        
        torch.cuda.empty_cache()
    
    logging.info(f"\n{'='*70}\n === HO√ÄN T·∫§T 'C√îNG X∆Ø·ªûNG' ADVANCED TS-GAN - ƒê√É ƒê√öC XONG C·∫¢ {len(ALL_LOOKBACKS)} N√ÉO! ===\n{'='*70}")
    
    if pynvml:
        try: pynvml.nvmlShutdown()
        except: pass